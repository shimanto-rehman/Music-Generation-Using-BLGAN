{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Music Generation Using BLGan.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLr9h1wwXC99",
        "outputId": "00054b5e-745c-4aa8-e2b5-b6e6bb47131f"
      },
      "source": [
        "pip install mido"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mido\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/0a/81beb587b1ae832ea6a1901dc7c6faa380e8dd154e0a862f0a9f3d2afab9/mido-1.2.9-py2.py3-none-any.whl (52kB)\n",
            "\r\u001b[K     |██████▎                         | 10kB 22.8MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 20kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 30kB 23.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 40kB 21.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 51kB 24.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 7.2MB/s \n",
            "\u001b[?25hInstalling collected packages: mido\n",
            "Successfully installed mido-1.2.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "q_wrOjjaGrJb"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Bidirectional, LSTM, Reshape, RepeatVector, TimeDistributed\n",
        "from keras.layers import BatchNormalization, Activation\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        " \n",
        "import mido # easy to use python MIDI library\n",
        "import os # accessing directory structure\n",
        "from __future__ import print_function, division\n",
        " \n",
        "from mido import MidiFile, MidiTrack, Message\n",
        " \n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "import sys\n",
        " \n",
        "import numpy as np\n",
        " \n",
        "import os\n",
        " \n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5tyvrY6WeEe",
        "outputId": "0508856e-adf2-4904-ffe1-dab10f34518c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fg9WOXW2Xk4O"
      },
      "source": [
        "paths = []\n",
        "songs = []\n",
        "\n",
        "for r, d, f in os.walk(r'/content/drive/MyDrive/Dataset/midi/blues'):\n",
        "    for file in f:\n",
        "        if '.mid' in file:\n",
        "            paths.append(os.path.join(r, file))\n",
        "\n",
        "for path in paths:\n",
        "    mid = MidiFile(path, type = 1)\n",
        "    songs.append(mid)\n",
        "del paths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dn5MST9kXzX9",
        "outputId": "42fff1ea-48b7-4801-fd55-a85dd522d75c"
      },
      "source": [
        "print(songs[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<midi file '/content/drive/MyDrive/Dataset/midi/blues/23_hours_blues_hh.mid' type 1, 1 tracks, 250 messages>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6K3Pn2mY2nR"
      },
      "source": [
        "notes = []\n",
        "dataset = []\n",
        "chunk = []\n",
        "\n",
        "for i in range(len(songs)):\n",
        "    for msg in songs[i]:\n",
        "        if not msg.is_meta:\n",
        "            if (msg.type == 'note_on'):\n",
        "                notes.append(msg.note)\n",
        "    for i in range(1, len(notes)):\n",
        "        chunk.append(notes[i])\n",
        "        if (i % 64 == 0):\n",
        "            dataset.append(chunk)\n",
        "            chunk = []\n",
        "    chunk = []\n",
        "    notes = []\n",
        "del chunk\n",
        "del notes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-4q3J3JgAlV",
        "outputId": "85e7e303-8cc0-4d02-9d9e-14922b7ee933"
      },
      "source": [
        "print(dataset[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[49, 37, 44, 44, 49, 44, 42, 37, 42, 37, 37, 44, 44, 49, 49, 46, 44, 44, 39, 42, 37, 37, 44, 44, 49, 44, 49, 44, 42, 49, 37, 44, 44, 49, 44, 42, 37, 42, 37, 37, 44, 44, 49, 49, 46, 44, 44, 39, 42, 37, 37, 44, 44, 49, 44, 37, 44, 44, 42, 37, 37, 44, 44, 49]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWZ7VanOYf3w",
        "outputId": "367a9bc5-9a85-4957-e404-38b38ee6477a"
      },
      "source": [
        "dataset = np.array(dataset)\n",
        "dataset.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(530, 64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "MDP7qnWQGrJu"
      },
      "source": [
        "def load_data():\n",
        "    x_train = dataset\n",
        "    x_train = x_train.reshape(530,8,8)\n",
        "    return x_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "-NIr-i_4GrJw"
      },
      "source": [
        "class LSTMGAN():\n",
        "    def __init__(self):\n",
        "        # Input shape\n",
        "        self.img_rows = 4\n",
        "        self.img_cols = 4\n",
        "        self.img_shape = (self.img_rows, self.img_cols)\n",
        "        self.latent_dim = 16\n",
        "\n",
        "        optimizer = Adam(0.0001, 0.4)\n",
        "\n",
        "        # Build the discriminator\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        self.discriminator.compile(loss='binary_crossentropy',\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # The generator takes noise as input and generates song\n",
        "        z = Input(shape=(4,4))\n",
        "        img = self.generator(z)\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # The discriminator takes generated images as input and determines validity\n",
        "        valid = self.discriminator(img)\n",
        "\n",
        "        # The combined model  (stacked generator and discriminator)\n",
        "        # Trains the generator to fool the discriminator\n",
        "        self.combined = Model(z, valid)\n",
        "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "    def build_generator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=(4, 4)))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Bidirectional(LSTM(128)))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        #specifying output to have 40 timesteps\n",
        "        model.add(RepeatVector(64))\n",
        "        #specifying 1 feature as the output\n",
        "        model.add(Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2)))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2)))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2)))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.3))   \n",
        "        model.add(TimeDistributed(Dense(128)))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.4))\n",
        "        model.add(TimeDistributed(Dense(128)))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.4))\n",
        "        model.add(TimeDistributed(Dense(1)))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.summary()\n",
        "\n",
        "        noise = Input(shape=(4,4))\n",
        "        img = model(noise)\n",
        "\n",
        "        return Model(noise, img)\n",
        "\n",
        "    def build_discriminator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Bidirectional(LSTM(128, activation = 'relu', return_sequences=True), input_shape=(64, 1)))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Bidirectional(LSTM(128, activation = 'relu')))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.4))\n",
        "        model.add(RepeatVector(4))\n",
        "        model.add(TimeDistributed(Dense(128, activation = 'sigmoid')))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.4))\n",
        "        model.add(TimeDistributed(Dense(128, activation = 'relu')))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.4))\n",
        "        model.add(TimeDistributed(Dense(1, activation = 'linear')))\n",
        "        model.summary()\n",
        "\n",
        "        img = Input(shape=(64,1))\n",
        "        validity = model(img)\n",
        "\n",
        "        return Model(img, validity)\n",
        "    \n",
        "\n",
        "    def train(self, epochs, batch_size=128, save_interval=50):\n",
        "\n",
        "        # Load the dataset\n",
        "        X_train = load_data()\n",
        "\n",
        "        # Rescale 0 to 1\n",
        "        X_train = X_train / 128\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = np.ones((batch_size,1,1))\n",
        "        fake = np.zeros((batch_size,1,1))\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            #  Train Discriminator\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "            imgs = X_train[idx]\n",
        "            imgs = np.array(imgs)\n",
        "            imgs = imgs.reshape(len(imgs),64,1)\n",
        "\n",
        "            # Sample noise and generate a batch of new songs\n",
        "            noise = np.random.normal(0, 1, (batch_size,4,4))\n",
        "            gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "            # Train the discriminator (real classified as ones and generated as zeros)\n",
        "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
        "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            # Train the generator\n",
        "            g_loss = self.combined.train_on_batch(noise, valid)\n",
        "\n",
        "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "\n",
        "            if epoch % save_interval == 0:\n",
        "                self.generator.save(\"/content/drive/MyDrive/AI Project/Models/LSTM_generator.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uY4sNenzGrJ9",
        "outputId": "d9017d8a-3081-4de1-8e1e-0f15793e8246"
      },
      "source": [
        "lstmgan = LSTMGAN()\n",
        "lstmgan.train(epochs=1000, batch_size=20, save_interval=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_8 (Bidirection (None, 64, 256)           133120    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_13 (LeakyReLU)   (None, 64, 256)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_9 (Bidirection (None, 256)               394240    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_14 (LeakyReLU)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "repeat_vector_2 (RepeatVecto (None, 4, 256)            0         \n",
            "_________________________________________________________________\n",
            "time_distributed_6 (TimeDist (None, 4, 128)            32896     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_15 (LeakyReLU)   (None, 4, 128)            0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 4, 128)            0         \n",
            "_________________________________________________________________\n",
            "time_distributed_7 (TimeDist (None, 4, 128)            16512     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_16 (LeakyReLU)   (None, 4, 128)            0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 4, 128)            0         \n",
            "_________________________________________________________________\n",
            "time_distributed_8 (TimeDist (None, 4, 1)              129       \n",
            "=================================================================\n",
            "Total params: 576,897\n",
            "Trainable params: 576,897\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_10 (Bidirectio (None, 4, 256)            136192    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_17 (LeakyReLU)   (None, 4, 256)            0         \n",
            "_________________________________________________________________\n",
            "bidirectional_11 (Bidirectio (None, 4, 256)            394240    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_18 (LeakyReLU)   (None, 4, 256)            0         \n",
            "_________________________________________________________________\n",
            "bidirectional_12 (Bidirectio (None, 256)               394240    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_19 (LeakyReLU)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "repeat_vector_3 (RepeatVecto (None, 64, 256)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_13 (Bidirectio (None, 64, 256)           394240    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_20 (LeakyReLU)   (None, 64, 256)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_14 (Bidirectio (None, 64, 256)           394240    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_21 (LeakyReLU)   (None, 64, 256)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_15 (Bidirectio (None, 64, 256)           394240    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_22 (LeakyReLU)   (None, 64, 256)           0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 64, 256)           0         \n",
            "_________________________________________________________________\n",
            "time_distributed_9 (TimeDist (None, 64, 128)           32896     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_23 (LeakyReLU)   (None, 64, 128)           0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 64, 128)           0         \n",
            "_________________________________________________________________\n",
            "time_distributed_10 (TimeDis (None, 64, 128)           16512     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_24 (LeakyReLU)   (None, 64, 128)           0         \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 64, 128)           0         \n",
            "_________________________________________________________________\n",
            "time_distributed_11 (TimeDis (None, 64, 1)             129       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_25 (LeakyReLU)   (None, 64, 1)             0         \n",
            "=================================================================\n",
            "Total params: 2,156,929\n",
            "Trainable params: 2,156,929\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:5 out of the last 5009 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f582efaea70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "0 [D loss: 5.847692, acc.: 49.37%] [G loss: 10.105593]\n",
            "1 [D loss: 5.611649, acc.: 51.25%] [G loss: 9.691597]\n",
            "2 [D loss: 4.637812, acc.: 47.50%] [G loss: 10.503078]\n",
            "3 [D loss: 5.055584, acc.: 44.37%] [G loss: 10.872991]\n",
            "4 [D loss: 5.252623, acc.: 50.62%] [G loss: 10.308093]\n",
            "5 [D loss: 5.376902, acc.: 53.75%] [G loss: 9.036832]\n",
            "6 [D loss: 5.020456, acc.: 47.50%] [G loss: 10.730681]\n",
            "7 [D loss: 3.723393, acc.: 53.75%] [G loss: 7.931251]\n",
            "8 [D loss: 5.395104, acc.: 48.12%] [G loss: 8.559869]\n",
            "9 [D loss: 4.934679, acc.: 45.00%] [G loss: 8.601336]\n",
            "10 [D loss: 5.364856, acc.: 46.25%] [G loss: 7.909731]\n",
            "11 [D loss: 4.355279, acc.: 55.00%] [G loss: 8.111212]\n",
            "12 [D loss: 4.355959, acc.: 51.87%] [G loss: 7.342723]\n",
            "13 [D loss: 5.603939, acc.: 46.88%] [G loss: 8.232852]\n",
            "14 [D loss: 5.510776, acc.: 46.87%] [G loss: 8.783148]\n",
            "15 [D loss: 4.786928, acc.: 46.25%] [G loss: 8.806236]\n",
            "16 [D loss: 5.631779, acc.: 45.00%] [G loss: 6.805211]\n",
            "17 [D loss: 4.607201, acc.: 46.87%] [G loss: 6.511680]\n",
            "18 [D loss: 4.992121, acc.: 50.00%] [G loss: 6.245659]\n",
            "19 [D loss: 4.842827, acc.: 49.38%] [G loss: 7.376722]\n",
            "20 [D loss: 4.809316, acc.: 49.37%] [G loss: 6.222126]\n",
            "21 [D loss: 4.228514, acc.: 48.12%] [G loss: 6.802821]\n",
            "22 [D loss: 4.158116, acc.: 50.00%] [G loss: 6.594470]\n",
            "23 [D loss: 3.876078, acc.: 53.75%] [G loss: 7.415227]\n",
            "24 [D loss: 4.143260, acc.: 54.37%] [G loss: 7.284447]\n",
            "25 [D loss: 4.316737, acc.: 52.50%] [G loss: 7.477226]\n",
            "26 [D loss: 4.000904, acc.: 55.62%] [G loss: 7.886638]\n",
            "27 [D loss: 4.059129, acc.: 50.00%] [G loss: 6.798199]\n",
            "28 [D loss: 3.826472, acc.: 51.25%] [G loss: 6.456513]\n",
            "29 [D loss: 4.698722, acc.: 46.25%] [G loss: 7.124708]\n",
            "30 [D loss: 3.975998, acc.: 47.50%] [G loss: 5.472921]\n",
            "31 [D loss: 4.043202, acc.: 50.63%] [G loss: 7.074805]\n",
            "32 [D loss: 4.292084, acc.: 51.88%] [G loss: 5.710759]\n",
            "33 [D loss: 4.433090, acc.: 43.75%] [G loss: 7.181915]\n",
            "34 [D loss: 3.872498, acc.: 49.37%] [G loss: 4.466461]\n",
            "35 [D loss: 4.393163, acc.: 51.88%] [G loss: 6.017789]\n",
            "36 [D loss: 4.427987, acc.: 48.13%] [G loss: 5.651688]\n",
            "37 [D loss: 4.204672, acc.: 44.37%] [G loss: 6.321934]\n",
            "38 [D loss: 4.387329, acc.: 48.13%] [G loss: 5.101932]\n",
            "39 [D loss: 3.780254, acc.: 55.00%] [G loss: 5.461492]\n",
            "40 [D loss: 3.755412, acc.: 57.50%] [G loss: 4.909321]\n",
            "41 [D loss: 4.856129, acc.: 45.00%] [G loss: 4.633780]\n",
            "42 [D loss: 4.081849, acc.: 58.75%] [G loss: 4.723850]\n",
            "43 [D loss: 4.237642, acc.: 49.37%] [G loss: 5.361230]\n",
            "44 [D loss: 4.236452, acc.: 47.50%] [G loss: 6.232749]\n",
            "45 [D loss: 5.178286, acc.: 43.12%] [G loss: 5.843532]\n",
            "46 [D loss: 3.535450, acc.: 55.00%] [G loss: 4.486156]\n",
            "47 [D loss: 3.918054, acc.: 46.87%] [G loss: 5.402296]\n",
            "48 [D loss: 4.416817, acc.: 45.62%] [G loss: 4.870568]\n",
            "49 [D loss: 3.948244, acc.: 53.12%] [G loss: 4.268946]\n",
            "50 [D loss: 3.946049, acc.: 52.50%] [G loss: 4.824491]\n",
            "51 [D loss: 3.898484, acc.: 53.12%] [G loss: 6.262880]\n",
            "52 [D loss: 2.708511, acc.: 47.50%] [G loss: 5.214833]\n",
            "53 [D loss: 4.059532, acc.: 48.13%] [G loss: 5.827757]\n",
            "54 [D loss: 4.443035, acc.: 42.50%] [G loss: 4.641519]\n",
            "55 [D loss: 4.102145, acc.: 46.88%] [G loss: 2.869300]\n",
            "56 [D loss: 3.398839, acc.: 51.25%] [G loss: 3.991250]\n",
            "57 [D loss: 3.698763, acc.: 47.50%] [G loss: 5.143853]\n",
            "58 [D loss: 4.132000, acc.: 54.38%] [G loss: 3.057679]\n",
            "59 [D loss: 4.475864, acc.: 50.00%] [G loss: 4.631091]\n",
            "60 [D loss: 4.739628, acc.: 47.50%] [G loss: 3.269399]\n",
            "61 [D loss: 4.552783, acc.: 51.25%] [G loss: 4.244890]\n",
            "62 [D loss: 3.128069, acc.: 57.50%] [G loss: 4.582015]\n",
            "63 [D loss: 4.007088, acc.: 51.25%] [G loss: 4.928819]\n",
            "64 [D loss: 3.151606, acc.: 50.00%] [G loss: 4.746280]\n",
            "65 [D loss: 3.878222, acc.: 50.00%] [G loss: 3.722003]\n",
            "66 [D loss: 3.916149, acc.: 46.88%] [G loss: 3.772777]\n",
            "67 [D loss: 4.499931, acc.: 49.37%] [G loss: 3.871401]\n",
            "68 [D loss: 4.516799, acc.: 51.87%] [G loss: 4.640579]\n",
            "69 [D loss: 3.490262, acc.: 55.00%] [G loss: 3.510305]\n",
            "70 [D loss: 3.975169, acc.: 52.50%] [G loss: 4.906500]\n",
            "71 [D loss: 3.843951, acc.: 49.37%] [G loss: 4.294032]\n",
            "72 [D loss: 3.650310, acc.: 48.12%] [G loss: 4.030249]\n",
            "73 [D loss: 4.279445, acc.: 41.88%] [G loss: 4.721015]\n",
            "74 [D loss: 4.643173, acc.: 42.50%] [G loss: 4.676939]\n",
            "75 [D loss: 3.567422, acc.: 55.63%] [G loss: 6.226829]\n",
            "76 [D loss: 3.228105, acc.: 53.75%] [G loss: 5.256912]\n",
            "77 [D loss: 3.736523, acc.: 54.37%] [G loss: 4.929411]\n",
            "78 [D loss: 4.284725, acc.: 47.50%] [G loss: 5.257305]\n",
            "79 [D loss: 4.571780, acc.: 51.87%] [G loss: 3.992939]\n",
            "80 [D loss: 3.784789, acc.: 48.75%] [G loss: 5.230990]\n",
            "81 [D loss: 4.273098, acc.: 52.50%] [G loss: 3.701526]\n",
            "82 [D loss: 3.690140, acc.: 48.75%] [G loss: 4.499185]\n",
            "83 [D loss: 4.104086, acc.: 50.62%] [G loss: 4.911729]\n",
            "84 [D loss: 3.526756, acc.: 45.62%] [G loss: 5.554023]\n",
            "85 [D loss: 4.125975, acc.: 48.12%] [G loss: 4.883540]\n",
            "86 [D loss: 4.074357, acc.: 45.00%] [G loss: 4.485314]\n",
            "87 [D loss: 3.484267, acc.: 55.00%] [G loss: 5.498716]\n",
            "88 [D loss: 3.060476, acc.: 53.13%] [G loss: 5.062331]\n",
            "89 [D loss: 4.311334, acc.: 45.62%] [G loss: 3.893183]\n",
            "90 [D loss: 4.373129, acc.: 48.13%] [G loss: 4.938291]\n",
            "91 [D loss: 4.087000, acc.: 51.25%] [G loss: 5.750953]\n",
            "92 [D loss: 4.059485, acc.: 50.00%] [G loss: 5.514321]\n",
            "93 [D loss: 4.208937, acc.: 56.87%] [G loss: 4.886064]\n",
            "94 [D loss: 4.287656, acc.: 46.25%] [G loss: 3.980268]\n",
            "95 [D loss: 3.838725, acc.: 49.37%] [G loss: 3.902953]\n",
            "96 [D loss: 4.017747, acc.: 48.75%] [G loss: 5.731141]\n",
            "97 [D loss: 3.450800, acc.: 56.25%] [G loss: 4.924927]\n",
            "98 [D loss: 3.567759, acc.: 46.88%] [G loss: 5.048313]\n",
            "99 [D loss: 3.898044, acc.: 51.25%] [G loss: 4.838166]\n",
            "100 [D loss: 4.106200, acc.: 47.50%] [G loss: 4.041708]\n",
            "101 [D loss: 3.698202, acc.: 47.50%] [G loss: 3.542041]\n",
            "102 [D loss: 3.130283, acc.: 48.75%] [G loss: 4.375072]\n",
            "103 [D loss: 3.170860, acc.: 50.00%] [G loss: 4.268554]\n",
            "104 [D loss: 3.344604, acc.: 48.12%] [G loss: 4.260830]\n",
            "105 [D loss: 4.047704, acc.: 51.25%] [G loss: 5.518313]\n",
            "106 [D loss: 3.783025, acc.: 54.38%] [G loss: 4.547254]\n",
            "107 [D loss: 3.471515, acc.: 51.25%] [G loss: 5.450407]\n",
            "108 [D loss: 4.555848, acc.: 47.50%] [G loss: 4.462808]\n",
            "109 [D loss: 3.276372, acc.: 55.63%] [G loss: 5.519036]\n",
            "110 [D loss: 4.286232, acc.: 51.25%] [G loss: 6.621391]\n",
            "111 [D loss: 3.490183, acc.: 52.50%] [G loss: 5.204055]\n",
            "112 [D loss: 3.367455, acc.: 47.50%] [G loss: 5.383864]\n",
            "113 [D loss: 4.389936, acc.: 47.50%] [G loss: 4.542120]\n",
            "114 [D loss: 3.381451, acc.: 55.00%] [G loss: 5.142839]\n",
            "115 [D loss: 3.264895, acc.: 54.38%] [G loss: 4.338489]\n",
            "116 [D loss: 3.252210, acc.: 51.25%] [G loss: 5.233134]\n",
            "117 [D loss: 4.403249, acc.: 39.37%] [G loss: 5.481307]\n",
            "118 [D loss: 3.896101, acc.: 51.87%] [G loss: 5.757684]\n",
            "119 [D loss: 3.679860, acc.: 52.50%] [G loss: 3.837123]\n",
            "120 [D loss: 3.040518, acc.: 52.50%] [G loss: 5.216969]\n",
            "121 [D loss: 3.790287, acc.: 51.87%] [G loss: 5.806262]\n",
            "122 [D loss: 2.922454, acc.: 51.87%] [G loss: 4.419948]\n",
            "123 [D loss: 3.699486, acc.: 53.13%] [G loss: 5.298635]\n",
            "124 [D loss: 4.074166, acc.: 45.00%] [G loss: 5.013059]\n",
            "125 [D loss: 2.938031, acc.: 48.75%] [G loss: 4.638720]\n",
            "126 [D loss: 3.527386, acc.: 50.00%] [G loss: 5.138037]\n",
            "127 [D loss: 4.483289, acc.: 46.88%] [G loss: 6.162089]\n",
            "128 [D loss: 3.155849, acc.: 54.37%] [G loss: 3.226928]\n",
            "129 [D loss: 3.704380, acc.: 49.37%] [G loss: 3.765530]\n",
            "130 [D loss: 4.294651, acc.: 48.12%] [G loss: 4.660771]\n",
            "131 [D loss: 3.318368, acc.: 44.38%] [G loss: 4.432887]\n",
            "132 [D loss: 2.944059, acc.: 53.75%] [G loss: 5.231386]\n",
            "133 [D loss: 3.737128, acc.: 50.62%] [G loss: 3.291509]\n",
            "134 [D loss: 4.041408, acc.: 43.75%] [G loss: 4.654980]\n",
            "135 [D loss: 3.240444, acc.: 55.63%] [G loss: 4.368798]\n",
            "136 [D loss: 4.503692, acc.: 41.88%] [G loss: 4.876882]\n",
            "137 [D loss: 3.396581, acc.: 55.00%] [G loss: 3.711915]\n",
            "138 [D loss: 3.061613, acc.: 50.00%] [G loss: 6.282382]\n",
            "139 [D loss: 4.572898, acc.: 47.50%] [G loss: 6.268443]\n",
            "140 [D loss: 3.398747, acc.: 50.00%] [G loss: 5.342542]\n",
            "141 [D loss: 4.491991, acc.: 44.38%] [G loss: 5.583298]\n",
            "142 [D loss: 4.305899, acc.: 47.50%] [G loss: 3.310664]\n",
            "143 [D loss: 3.531248, acc.: 53.75%] [G loss: 4.051167]\n",
            "144 [D loss: 4.245133, acc.: 45.00%] [G loss: 5.404334]\n",
            "145 [D loss: 4.227304, acc.: 43.75%] [G loss: 3.924278]\n",
            "146 [D loss: 3.744179, acc.: 47.50%] [G loss: 3.842541]\n",
            "147 [D loss: 3.296094, acc.: 51.88%] [G loss: 3.222985]\n",
            "148 [D loss: 4.408357, acc.: 45.00%] [G loss: 3.901249]\n",
            "149 [D loss: 3.409262, acc.: 48.13%] [G loss: 5.399164]\n",
            "150 [D loss: 3.847145, acc.: 45.62%] [G loss: 3.235878]\n",
            "151 [D loss: 3.566882, acc.: 43.75%] [G loss: 3.859170]\n",
            "152 [D loss: 4.526631, acc.: 46.25%] [G loss: 4.454846]\n",
            "153 [D loss: 3.970034, acc.: 45.63%] [G loss: 2.710477]\n",
            "154 [D loss: 3.759042, acc.: 47.50%] [G loss: 2.741018]\n",
            "155 [D loss: 3.154708, acc.: 53.75%] [G loss: 3.174567]\n",
            "156 [D loss: 3.037176, acc.: 53.75%] [G loss: 3.799610]\n",
            "157 [D loss: 3.899794, acc.: 50.00%] [G loss: 2.596514]\n",
            "158 [D loss: 3.345081, acc.: 56.87%] [G loss: 4.163263]\n",
            "159 [D loss: 3.563701, acc.: 55.00%] [G loss: 3.185536]\n",
            "160 [D loss: 3.836717, acc.: 50.00%] [G loss: 3.222203]\n",
            "161 [D loss: 3.945323, acc.: 50.62%] [G loss: 4.154523]\n",
            "162 [D loss: 3.384400, acc.: 53.12%] [G loss: 3.811557]\n",
            "163 [D loss: 3.796929, acc.: 43.13%] [G loss: 3.286021]\n",
            "164 [D loss: 4.124930, acc.: 45.62%] [G loss: 4.617804]\n",
            "165 [D loss: 3.060522, acc.: 51.88%] [G loss: 4.018511]\n",
            "166 [D loss: 3.642520, acc.: 53.75%] [G loss: 3.369012]\n",
            "167 [D loss: 3.181182, acc.: 47.50%] [G loss: 5.416184]\n",
            "168 [D loss: 3.081130, acc.: 52.50%] [G loss: 3.145935]\n",
            "169 [D loss: 3.417795, acc.: 55.63%] [G loss: 3.610772]\n",
            "170 [D loss: 3.406660, acc.: 54.38%] [G loss: 3.525278]\n",
            "171 [D loss: 3.899968, acc.: 48.75%] [G loss: 3.966326]\n",
            "172 [D loss: 3.575182, acc.: 53.13%] [G loss: 3.885237]\n",
            "173 [D loss: 3.370541, acc.: 47.50%] [G loss: 3.075472]\n",
            "174 [D loss: 3.745994, acc.: 52.50%] [G loss: 2.332231]\n",
            "175 [D loss: 4.049684, acc.: 40.00%] [G loss: 4.110293]\n",
            "176 [D loss: 3.015823, acc.: 55.00%] [G loss: 3.380797]\n",
            "177 [D loss: 2.979655, acc.: 53.75%] [G loss: 4.035665]\n",
            "178 [D loss: 3.012200, acc.: 53.12%] [G loss: 4.668305]\n",
            "179 [D loss: 3.330728, acc.: 46.87%] [G loss: 4.899890]\n",
            "180 [D loss: 3.037149, acc.: 51.25%] [G loss: 2.810885]\n",
            "181 [D loss: 3.259895, acc.: 51.25%] [G loss: 3.822364]\n",
            "182 [D loss: 3.257444, acc.: 50.62%] [G loss: 3.267370]\n",
            "183 [D loss: 3.799217, acc.: 46.88%] [G loss: 3.998903]\n",
            "184 [D loss: 3.178059, acc.: 46.25%] [G loss: 3.087351]\n",
            "185 [D loss: 3.474012, acc.: 52.50%] [G loss: 4.150360]\n",
            "186 [D loss: 3.707303, acc.: 46.88%] [G loss: 4.165744]\n",
            "187 [D loss: 3.685044, acc.: 53.12%] [G loss: 4.321666]\n",
            "188 [D loss: 3.914284, acc.: 52.50%] [G loss: 4.566532]\n",
            "189 [D loss: 3.684628, acc.: 46.25%] [G loss: 4.942956]\n",
            "190 [D loss: 2.628149, acc.: 49.38%] [G loss: 3.926720]\n",
            "191 [D loss: 3.640258, acc.: 50.00%] [G loss: 4.287434]\n",
            "192 [D loss: 2.471655, acc.: 59.38%] [G loss: 5.870815]\n",
            "193 [D loss: 3.899902, acc.: 50.62%] [G loss: 5.021464]\n",
            "194 [D loss: 3.540402, acc.: 45.00%] [G loss: 3.961369]\n",
            "195 [D loss: 3.541047, acc.: 49.37%] [G loss: 5.321578]\n",
            "196 [D loss: 2.878489, acc.: 50.63%] [G loss: 4.592304]\n",
            "197 [D loss: 3.446709, acc.: 48.75%] [G loss: 5.274111]\n",
            "198 [D loss: 3.898302, acc.: 51.25%] [G loss: 5.221036]\n",
            "199 [D loss: 3.780649, acc.: 49.37%] [G loss: 4.344304]\n",
            "200 [D loss: 4.442622, acc.: 43.12%] [G loss: 3.587231]\n",
            "201 [D loss: 3.151386, acc.: 52.50%] [G loss: 5.595937]\n",
            "202 [D loss: 3.850260, acc.: 48.12%] [G loss: 5.141154]\n",
            "203 [D loss: 3.410793, acc.: 51.88%] [G loss: 4.248521]\n",
            "204 [D loss: 3.996245, acc.: 47.50%] [G loss: 4.048961]\n",
            "205 [D loss: 2.895566, acc.: 53.13%] [G loss: 3.823444]\n",
            "206 [D loss: 4.010679, acc.: 45.62%] [G loss: 4.008521]\n",
            "207 [D loss: 3.660006, acc.: 50.63%] [G loss: 5.165777]\n",
            "208 [D loss: 3.685894, acc.: 50.62%] [G loss: 4.438042]\n",
            "209 [D loss: 3.278735, acc.: 55.00%] [G loss: 4.416997]\n",
            "210 [D loss: 4.322351, acc.: 42.50%] [G loss: 3.662235]\n",
            "211 [D loss: 3.221032, acc.: 55.00%] [G loss: 4.942297]\n",
            "212 [D loss: 3.298732, acc.: 55.00%] [G loss: 4.650430]\n",
            "213 [D loss: 3.572393, acc.: 43.75%] [G loss: 3.853899]\n",
            "214 [D loss: 3.559837, acc.: 51.25%] [G loss: 4.386931]\n",
            "215 [D loss: 3.996413, acc.: 44.37%] [G loss: 3.083002]\n",
            "216 [D loss: 3.370813, acc.: 49.38%] [G loss: 4.821452]\n",
            "217 [D loss: 2.586320, acc.: 51.25%] [G loss: 4.040396]\n",
            "218 [D loss: 3.631238, acc.: 51.25%] [G loss: 4.373178]\n",
            "219 [D loss: 3.036976, acc.: 50.63%] [G loss: 3.682683]\n",
            "220 [D loss: 3.230701, acc.: 45.62%] [G loss: 4.960785]\n",
            "221 [D loss: 4.209692, acc.: 50.63%] [G loss: 5.008384]\n",
            "222 [D loss: 3.962406, acc.: 45.00%] [G loss: 4.076298]\n",
            "223 [D loss: 3.247282, acc.: 51.87%] [G loss: 4.461708]\n",
            "224 [D loss: 3.492063, acc.: 46.88%] [G loss: 3.266811]\n",
            "225 [D loss: 2.924783, acc.: 50.62%] [G loss: 4.474080]\n",
            "226 [D loss: 3.354403, acc.: 42.50%] [G loss: 3.350118]\n",
            "227 [D loss: 3.502348, acc.: 48.13%] [G loss: 3.319125]\n",
            "228 [D loss: 3.232693, acc.: 43.75%] [G loss: 3.811675]\n",
            "229 [D loss: 3.959290, acc.: 48.13%] [G loss: 3.951611]\n",
            "230 [D loss: 4.073330, acc.: 47.50%] [G loss: 4.118673]\n",
            "231 [D loss: 3.023318, acc.: 51.25%] [G loss: 3.323576]\n",
            "232 [D loss: 3.264963, acc.: 55.63%] [G loss: 5.026165]\n",
            "233 [D loss: 3.990196, acc.: 43.75%] [G loss: 3.184535]\n",
            "234 [D loss: 3.860739, acc.: 50.00%] [G loss: 3.366503]\n",
            "235 [D loss: 3.213237, acc.: 50.63%] [G loss: 4.627299]\n",
            "236 [D loss: 4.301158, acc.: 45.62%] [G loss: 3.547325]\n",
            "237 [D loss: 2.961238, acc.: 48.75%] [G loss: 2.278744]\n",
            "238 [D loss: 2.956386, acc.: 51.25%] [G loss: 2.415886]\n",
            "239 [D loss: 3.440959, acc.: 47.50%] [G loss: 2.759338]\n",
            "240 [D loss: 3.878566, acc.: 50.00%] [G loss: 2.131849]\n",
            "241 [D loss: 3.959868, acc.: 51.25%] [G loss: 1.528916]\n",
            "242 [D loss: 3.391014, acc.: 49.37%] [G loss: 3.070419]\n",
            "243 [D loss: 3.215680, acc.: 56.88%] [G loss: 2.594162]\n",
            "244 [D loss: 3.019820, acc.: 48.12%] [G loss: 3.507634]\n",
            "245 [D loss: 3.508543, acc.: 55.63%] [G loss: 3.757770]\n",
            "246 [D loss: 3.604399, acc.: 49.38%] [G loss: 2.887772]\n",
            "247 [D loss: 2.811875, acc.: 54.38%] [G loss: 2.893546]\n",
            "248 [D loss: 3.717986, acc.: 46.88%] [G loss: 3.971410]\n",
            "249 [D loss: 3.243343, acc.: 45.62%] [G loss: 4.023229]\n",
            "250 [D loss: 3.833197, acc.: 48.13%] [G loss: 3.227139]\n",
            "251 [D loss: 2.808697, acc.: 53.75%] [G loss: 3.103170]\n",
            "252 [D loss: 3.356524, acc.: 53.12%] [G loss: 3.656295]\n",
            "253 [D loss: 2.801621, acc.: 48.12%] [G loss: 3.862201]\n",
            "254 [D loss: 3.019795, acc.: 48.75%] [G loss: 3.219267]\n",
            "255 [D loss: 3.265968, acc.: 45.62%] [G loss: 2.961004]\n",
            "256 [D loss: 3.298324, acc.: 51.25%] [G loss: 3.353872]\n",
            "257 [D loss: 3.267142, acc.: 51.87%] [G loss: 3.246817]\n",
            "258 [D loss: 2.592642, acc.: 56.25%] [G loss: 2.774295]\n",
            "259 [D loss: 2.724021, acc.: 57.50%] [G loss: 2.952298]\n",
            "260 [D loss: 3.680121, acc.: 49.38%] [G loss: 3.923903]\n",
            "261 [D loss: 3.482704, acc.: 45.62%] [G loss: 3.075343]\n",
            "262 [D loss: 3.654239, acc.: 50.00%] [G loss: 2.474433]\n",
            "263 [D loss: 3.090016, acc.: 53.75%] [G loss: 12.140158]\n",
            "264 [D loss: 4.989534, acc.: 50.62%] [G loss: 12.579412]\n",
            "265 [D loss: 5.309502, acc.: 51.88%] [G loss: 11.752617]\n",
            "266 [D loss: 5.721534, acc.: 51.88%] [G loss: 10.268549]\n",
            "267 [D loss: 5.374858, acc.: 53.75%] [G loss: 10.880747]\n",
            "268 [D loss: 5.100705, acc.: 53.13%] [G loss: 11.056648]\n",
            "269 [D loss: 6.050099, acc.: 35.63%] [G loss: 5.515070]\n",
            "270 [D loss: 3.269973, acc.: 55.00%] [G loss: 4.507595]\n",
            "271 [D loss: 2.610536, acc.: 51.25%] [G loss: 5.249255]\n",
            "272 [D loss: 2.529244, acc.: 50.62%] [G loss: 5.005998]\n",
            "273 [D loss: 3.691142, acc.: 48.75%] [G loss: 4.834686]\n",
            "274 [D loss: 3.327410, acc.: 48.13%] [G loss: 5.736635]\n",
            "275 [D loss: 2.911138, acc.: 51.87%] [G loss: 4.402025]\n",
            "276 [D loss: 3.834698, acc.: 46.25%] [G loss: 3.972039]\n",
            "277 [D loss: 3.121284, acc.: 46.88%] [G loss: 5.139661]\n",
            "278 [D loss: 2.894408, acc.: 53.75%] [G loss: 4.212497]\n",
            "279 [D loss: 3.022693, acc.: 45.00%] [G loss: 5.211174]\n",
            "280 [D loss: 3.481108, acc.: 44.37%] [G loss: 4.997702]\n",
            "281 [D loss: 3.245833, acc.: 57.50%] [G loss: 5.133609]\n",
            "282 [D loss: 2.565541, acc.: 55.63%] [G loss: 6.386046]\n",
            "283 [D loss: 3.351896, acc.: 46.25%] [G loss: 6.240388]\n",
            "284 [D loss: 3.070682, acc.: 55.62%] [G loss: 4.744582]\n",
            "285 [D loss: 3.333746, acc.: 50.62%] [G loss: 3.909619]\n",
            "286 [D loss: 2.548272, acc.: 50.00%] [G loss: 3.969409]\n",
            "287 [D loss: 2.442443, acc.: 56.87%] [G loss: 6.240869]\n",
            "288 [D loss: 3.977115, acc.: 50.00%] [G loss: 5.789150]\n",
            "289 [D loss: 4.223127, acc.: 50.00%] [G loss: 4.576789]\n",
            "290 [D loss: 3.181730, acc.: 44.37%] [G loss: 4.028216]\n",
            "291 [D loss: 3.655345, acc.: 41.88%] [G loss: 4.605923]\n",
            "292 [D loss: 3.483197, acc.: 50.62%] [G loss: 4.871027]\n",
            "293 [D loss: 3.382426, acc.: 50.63%] [G loss: 4.454880]\n",
            "294 [D loss: 2.853420, acc.: 47.50%] [G loss: 4.111732]\n",
            "295 [D loss: 3.568252, acc.: 46.25%] [G loss: 4.513072]\n",
            "296 [D loss: 3.667352, acc.: 41.25%] [G loss: 4.628787]\n",
            "297 [D loss: 3.574311, acc.: 41.25%] [G loss: 4.376933]\n",
            "298 [D loss: 2.490666, acc.: 48.75%] [G loss: 4.535376]\n",
            "299 [D loss: 2.941943, acc.: 48.75%] [G loss: 4.647161]\n",
            "300 [D loss: 3.001873, acc.: 50.00%] [G loss: 4.418455]\n",
            "301 [D loss: 4.007571, acc.: 48.75%] [G loss: 4.538697]\n",
            "302 [D loss: 3.757875, acc.: 38.75%] [G loss: 3.074739]\n",
            "303 [D loss: 3.285581, acc.: 53.12%] [G loss: 4.395492]\n",
            "304 [D loss: 3.881696, acc.: 51.25%] [G loss: 3.424665]\n",
            "305 [D loss: 2.809492, acc.: 48.75%] [G loss: 4.964678]\n",
            "306 [D loss: 3.381859, acc.: 43.12%] [G loss: 5.071256]\n",
            "307 [D loss: 2.624407, acc.: 51.88%] [G loss: 3.015405]\n",
            "308 [D loss: 4.078761, acc.: 41.25%] [G loss: 5.162462]\n",
            "309 [D loss: 3.055917, acc.: 48.12%] [G loss: 3.409924]\n",
            "310 [D loss: 3.313590, acc.: 44.38%] [G loss: 4.585979]\n",
            "311 [D loss: 2.794968, acc.: 55.63%] [G loss: 4.603585]\n",
            "312 [D loss: 2.972346, acc.: 50.63%] [G loss: 4.620579]\n",
            "313 [D loss: 3.528782, acc.: 50.00%] [G loss: 5.061004]\n",
            "314 [D loss: 3.355618, acc.: 50.00%] [G loss: 4.949514]\n",
            "315 [D loss: 3.101138, acc.: 56.25%] [G loss: 3.419465]\n",
            "316 [D loss: 2.577805, acc.: 55.62%] [G loss: 3.528666]\n",
            "317 [D loss: 3.188170, acc.: 53.75%] [G loss: 3.077385]\n",
            "318 [D loss: 3.196568, acc.: 52.50%] [G loss: 4.440618]\n",
            "319 [D loss: 2.888480, acc.: 46.25%] [G loss: 3.129170]\n",
            "320 [D loss: 3.473415, acc.: 49.38%] [G loss: 4.058892]\n",
            "321 [D loss: 3.287461, acc.: 50.00%] [G loss: 5.628739]\n",
            "322 [D loss: 4.167732, acc.: 50.63%] [G loss: 3.790438]\n",
            "323 [D loss: 2.756624, acc.: 53.75%] [G loss: 5.134166]\n",
            "324 [D loss: 3.110822, acc.: 51.25%] [G loss: 5.793468]\n",
            "325 [D loss: 2.608847, acc.: 55.63%] [G loss: 6.197740]\n",
            "326 [D loss: 3.488001, acc.: 54.37%] [G loss: 5.409316]\n",
            "327 [D loss: 5.236312, acc.: 46.25%] [G loss: 4.384374]\n",
            "328 [D loss: 2.969493, acc.: 56.87%] [G loss: 4.366696]\n",
            "329 [D loss: 2.747528, acc.: 51.25%] [G loss: 3.874151]\n",
            "330 [D loss: 3.189218, acc.: 51.25%] [G loss: 5.658808]\n",
            "331 [D loss: 2.479048, acc.: 55.63%] [G loss: 3.467957]\n",
            "332 [D loss: 3.305223, acc.: 47.50%] [G loss: 4.061753]\n",
            "333 [D loss: 3.415776, acc.: 42.50%] [G loss: 4.989354]\n",
            "334 [D loss: 3.389498, acc.: 55.00%] [G loss: 4.331580]\n",
            "335 [D loss: 3.207272, acc.: 53.12%] [G loss: 4.413181]\n",
            "336 [D loss: 2.837412, acc.: 53.13%] [G loss: 3.669916]\n",
            "337 [D loss: 3.191205, acc.: 41.88%] [G loss: 2.563639]\n",
            "338 [D loss: 3.267681, acc.: 46.25%] [G loss: 2.290187]\n",
            "339 [D loss: 2.406681, acc.: 51.25%] [G loss: 2.989038]\n",
            "340 [D loss: 3.884683, acc.: 47.50%] [G loss: 3.266146]\n",
            "341 [D loss: 1.853325, acc.: 49.38%] [G loss: 4.398647]\n",
            "342 [D loss: 3.021819, acc.: 58.75%] [G loss: 5.206313]\n",
            "343 [D loss: 3.005751, acc.: 48.13%] [G loss: 5.625943]\n",
            "344 [D loss: 3.117499, acc.: 53.12%] [G loss: 6.012388]\n",
            "345 [D loss: 2.785863, acc.: 56.88%] [G loss: 4.743227]\n",
            "346 [D loss: 3.575522, acc.: 48.75%] [G loss: 3.608163]\n",
            "347 [D loss: 3.971082, acc.: 45.00%] [G loss: 3.243321]\n",
            "348 [D loss: 4.690740, acc.: 42.50%] [G loss: 3.535021]\n",
            "349 [D loss: 3.946948, acc.: 43.75%] [G loss: 5.314026]\n",
            "350 [D loss: 3.939053, acc.: 43.13%] [G loss: 4.364891]\n",
            "351 [D loss: 3.142504, acc.: 48.75%] [G loss: 3.731729]\n",
            "352 [D loss: 3.308743, acc.: 53.75%] [G loss: 4.082807]\n",
            "353 [D loss: 2.455371, acc.: 54.37%] [G loss: 4.666170]\n",
            "354 [D loss: 2.818482, acc.: 50.00%] [G loss: 3.926640]\n",
            "355 [D loss: 3.280595, acc.: 54.37%] [G loss: 5.058359]\n",
            "356 [D loss: 2.806176, acc.: 45.00%] [G loss: 4.376609]\n",
            "357 [D loss: 3.039554, acc.: 45.62%] [G loss: 3.464249]\n",
            "358 [D loss: 2.236600, acc.: 50.00%] [G loss: 4.186080]\n",
            "359 [D loss: 3.239545, acc.: 54.38%] [G loss: 2.931234]\n",
            "360 [D loss: 3.015904, acc.: 48.75%] [G loss: 3.173310]\n",
            "361 [D loss: 3.488149, acc.: 46.25%] [G loss: 4.121882]\n",
            "362 [D loss: 2.550945, acc.: 51.25%] [G loss: 4.318069]\n",
            "363 [D loss: 3.205398, acc.: 50.00%] [G loss: 2.488689]\n",
            "364 [D loss: 2.751394, acc.: 50.62%] [G loss: 3.511811]\n",
            "365 [D loss: 2.743544, acc.: 52.50%] [G loss: 4.801253]\n",
            "366 [D loss: 3.243922, acc.: 48.75%] [G loss: 4.001322]\n",
            "367 [D loss: 2.855933, acc.: 51.25%] [G loss: 2.713903]\n",
            "368 [D loss: 2.264986, acc.: 50.63%] [G loss: 3.719932]\n",
            "369 [D loss: 3.028016, acc.: 50.62%] [G loss: 3.217403]\n",
            "370 [D loss: 3.581797, acc.: 41.87%] [G loss: 3.040429]\n",
            "371 [D loss: 2.835248, acc.: 53.12%] [G loss: 2.209515]\n",
            "372 [D loss: 2.806443, acc.: 53.75%] [G loss: 3.988761]\n",
            "373 [D loss: 2.881821, acc.: 48.75%] [G loss: 3.557921]\n",
            "374 [D loss: 2.364990, acc.: 50.00%] [G loss: 3.263793]\n",
            "375 [D loss: 3.111731, acc.: 50.00%] [G loss: 3.871200]\n",
            "376 [D loss: 3.402358, acc.: 45.62%] [G loss: 3.096566]\n",
            "377 [D loss: 2.253415, acc.: 57.50%] [G loss: 2.672438]\n",
            "378 [D loss: 3.606167, acc.: 43.75%] [G loss: 3.119847]\n",
            "379 [D loss: 3.166182, acc.: 46.25%] [G loss: 2.970649]\n",
            "380 [D loss: 2.151134, acc.: 54.37%] [G loss: 2.760760]\n",
            "381 [D loss: 2.530976, acc.: 62.50%] [G loss: 2.537693]\n",
            "382 [D loss: 2.382148, acc.: 54.37%] [G loss: 3.974370]\n",
            "383 [D loss: 2.549788, acc.: 46.88%] [G loss: 2.692719]\n",
            "384 [D loss: 3.750201, acc.: 45.63%] [G loss: 2.545703]\n",
            "385 [D loss: 2.959608, acc.: 46.88%] [G loss: 2.032597]\n",
            "386 [D loss: 2.553547, acc.: 51.25%] [G loss: 3.256699]\n",
            "387 [D loss: 2.810376, acc.: 46.88%] [G loss: 2.546017]\n",
            "388 [D loss: 2.878570, acc.: 45.63%] [G loss: 1.888339]\n",
            "389 [D loss: 2.902090, acc.: 49.38%] [G loss: 2.596250]\n",
            "390 [D loss: 2.672449, acc.: 52.50%] [G loss: 2.408974]\n",
            "391 [D loss: 3.315946, acc.: 51.25%] [G loss: 2.637404]\n",
            "392 [D loss: 2.284737, acc.: 55.00%] [G loss: 2.561187]\n",
            "393 [D loss: 2.807209, acc.: 51.25%] [G loss: 1.883144]\n",
            "394 [D loss: 2.572554, acc.: 52.50%] [G loss: 3.221767]\n",
            "395 [D loss: 4.382372, acc.: 40.00%] [G loss: 2.433799]\n",
            "396 [D loss: 2.652537, acc.: 55.00%] [G loss: 2.052706]\n",
            "397 [D loss: 3.248129, acc.: 48.75%] [G loss: 2.037887]\n",
            "398 [D loss: 2.433401, acc.: 53.12%] [G loss: 3.766634]\n",
            "399 [D loss: 3.012373, acc.: 47.50%] [G loss: 2.927102]\n",
            "400 [D loss: 3.296180, acc.: 50.00%] [G loss: 2.272609]\n",
            "401 [D loss: 2.805841, acc.: 53.13%] [G loss: 2.524031]\n",
            "402 [D loss: 2.483201, acc.: 51.25%] [G loss: 2.023198]\n",
            "403 [D loss: 2.773830, acc.: 58.75%] [G loss: 1.788831]\n",
            "404 [D loss: 2.898511, acc.: 48.75%] [G loss: 2.690299]\n",
            "405 [D loss: 2.687414, acc.: 56.25%] [G loss: 3.491419]\n",
            "406 [D loss: 3.456198, acc.: 46.25%] [G loss: 2.874188]\n",
            "407 [D loss: 3.151995, acc.: 43.75%] [G loss: 1.959479]\n",
            "408 [D loss: 2.724364, acc.: 45.00%] [G loss: 1.933722]\n",
            "409 [D loss: 2.440932, acc.: 44.37%] [G loss: 4.120770]\n",
            "410 [D loss: 2.081161, acc.: 56.88%] [G loss: 4.099067]\n",
            "411 [D loss: 3.361011, acc.: 44.37%] [G loss: 4.433171]\n",
            "412 [D loss: 3.801930, acc.: 43.75%] [G loss: 3.982075]\n",
            "413 [D loss: 2.798065, acc.: 56.25%] [G loss: 3.626652]\n",
            "414 [D loss: 3.647821, acc.: 50.62%] [G loss: 4.035762]\n",
            "415 [D loss: 3.439802, acc.: 50.00%] [G loss: 4.545686]\n",
            "416 [D loss: 2.905451, acc.: 52.50%] [G loss: 4.135638]\n",
            "417 [D loss: 2.788687, acc.: 48.13%] [G loss: 5.543239]\n",
            "418 [D loss: 2.776927, acc.: 47.50%] [G loss: 5.541982]\n",
            "419 [D loss: 2.321969, acc.: 54.37%] [G loss: 4.727736]\n",
            "420 [D loss: 3.443014, acc.: 47.50%] [G loss: 3.739548]\n",
            "421 [D loss: 2.802052, acc.: 53.12%] [G loss: 3.596015]\n",
            "422 [D loss: 2.920068, acc.: 45.00%] [G loss: 4.389775]\n",
            "423 [D loss: 2.452428, acc.: 48.75%] [G loss: 3.570897]\n",
            "424 [D loss: 3.961303, acc.: 50.62%] [G loss: 3.218973]\n",
            "425 [D loss: 3.080134, acc.: 51.25%] [G loss: 4.570342]\n",
            "426 [D loss: 3.413242, acc.: 45.63%] [G loss: 3.593754]\n",
            "427 [D loss: 2.523374, acc.: 51.25%] [G loss: 3.942112]\n",
            "428 [D loss: 2.743011, acc.: 53.75%] [G loss: 2.811050]\n",
            "429 [D loss: 3.134785, acc.: 53.75%] [G loss: 2.895030]\n",
            "430 [D loss: 3.071039, acc.: 41.25%] [G loss: 3.487614]\n",
            "431 [D loss: 2.999569, acc.: 50.62%] [G loss: 5.168441]\n",
            "432 [D loss: 3.046127, acc.: 44.38%] [G loss: 6.374270]\n",
            "433 [D loss: 4.108739, acc.: 45.62%] [G loss: 4.938253]\n",
            "434 [D loss: 2.939031, acc.: 49.38%] [G loss: 3.919768]\n",
            "435 [D loss: 3.434594, acc.: 47.50%] [G loss: 4.725684]\n",
            "436 [D loss: 3.299760, acc.: 44.37%] [G loss: 5.211179]\n",
            "437 [D loss: 3.135714, acc.: 52.50%] [G loss: 4.292233]\n",
            "438 [D loss: 3.198721, acc.: 45.62%] [G loss: 4.933305]\n",
            "439 [D loss: 2.985230, acc.: 45.00%] [G loss: 4.837598]\n",
            "440 [D loss: 3.030068, acc.: 53.75%] [G loss: 4.522146]\n",
            "441 [D loss: 3.522147, acc.: 48.75%] [G loss: 5.426751]\n",
            "442 [D loss: 3.168305, acc.: 48.12%] [G loss: 4.584674]\n",
            "443 [D loss: 3.439403, acc.: 46.25%] [G loss: 5.745437]\n",
            "444 [D loss: 3.347604, acc.: 49.38%] [G loss: 5.010156]\n",
            "445 [D loss: 2.879040, acc.: 52.50%] [G loss: 5.205844]\n",
            "446 [D loss: 3.576318, acc.: 46.88%] [G loss: 4.206157]\n",
            "447 [D loss: 2.730276, acc.: 50.62%] [G loss: 4.455589]\n",
            "448 [D loss: 2.915745, acc.: 51.87%] [G loss: 5.994230]\n",
            "449 [D loss: 3.419128, acc.: 50.62%] [G loss: 3.746248]\n",
            "450 [D loss: 2.943181, acc.: 44.38%] [G loss: 4.153274]\n",
            "451 [D loss: 2.802850, acc.: 53.12%] [G loss: 4.399322]\n",
            "452 [D loss: 3.348811, acc.: 50.63%] [G loss: 4.173431]\n",
            "453 [D loss: 2.726573, acc.: 51.25%] [G loss: 3.977196]\n",
            "454 [D loss: 3.048714, acc.: 50.00%] [G loss: 4.451077]\n",
            "455 [D loss: 2.042438, acc.: 46.25%] [G loss: 3.760461]\n",
            "456 [D loss: 3.017567, acc.: 51.87%] [G loss: 3.616234]\n",
            "457 [D loss: 2.611586, acc.: 52.50%] [G loss: 4.082710]\n",
            "458 [D loss: 2.393003, acc.: 53.12%] [G loss: 4.211927]\n",
            "459 [D loss: 3.155867, acc.: 48.13%] [G loss: 3.739384]\n",
            "460 [D loss: 2.346067, acc.: 56.88%] [G loss: 3.016808]\n",
            "461 [D loss: 3.201341, acc.: 51.87%] [G loss: 4.360822]\n",
            "462 [D loss: 2.745842, acc.: 51.87%] [G loss: 4.399839]\n",
            "463 [D loss: 3.467692, acc.: 43.75%] [G loss: 4.809714]\n",
            "464 [D loss: 2.413959, acc.: 52.50%] [G loss: 2.690676]\n",
            "465 [D loss: 2.712630, acc.: 56.25%] [G loss: 3.621062]\n",
            "466 [D loss: 3.170817, acc.: 52.50%] [G loss: 3.122006]\n",
            "467 [D loss: 2.768696, acc.: 49.37%] [G loss: 4.026114]\n",
            "468 [D loss: 2.601126, acc.: 54.37%] [G loss: 3.643999]\n",
            "469 [D loss: 2.680238, acc.: 46.88%] [G loss: 2.978984]\n",
            "470 [D loss: 2.426506, acc.: 53.75%] [G loss: 4.464730]\n",
            "471 [D loss: 2.584420, acc.: 48.12%] [G loss: 4.824456]\n",
            "472 [D loss: 2.075974, acc.: 56.87%] [G loss: 3.817642]\n",
            "473 [D loss: 3.046361, acc.: 50.00%] [G loss: 3.040782]\n",
            "474 [D loss: 3.076779, acc.: 48.13%] [G loss: 3.701305]\n",
            "475 [D loss: 3.100866, acc.: 53.75%] [G loss: 3.389448]\n",
            "476 [D loss: 2.525606, acc.: 55.63%] [G loss: 3.418513]\n",
            "477 [D loss: 2.993289, acc.: 54.38%] [G loss: 4.824187]\n",
            "478 [D loss: 3.228277, acc.: 45.00%] [G loss: 3.439093]\n",
            "479 [D loss: 2.453721, acc.: 49.38%] [G loss: 3.252002]\n",
            "480 [D loss: 2.354805, acc.: 53.75%] [G loss: 3.924603]\n",
            "481 [D loss: 2.772404, acc.: 44.37%] [G loss: 3.898564]\n",
            "482 [D loss: 3.002961, acc.: 51.25%] [G loss: 4.324174]\n",
            "483 [D loss: 3.169095, acc.: 45.00%] [G loss: 4.401947]\n",
            "484 [D loss: 2.598225, acc.: 46.88%] [G loss: 3.427219]\n",
            "485 [D loss: 2.373461, acc.: 54.38%] [G loss: 3.367730]\n",
            "486 [D loss: 2.683762, acc.: 51.25%] [G loss: 2.965087]\n",
            "487 [D loss: 3.063934, acc.: 50.00%] [G loss: 3.404284]\n",
            "488 [D loss: 2.250761, acc.: 51.25%] [G loss: 2.362595]\n",
            "489 [D loss: 3.228970, acc.: 45.63%] [G loss: 3.064038]\n",
            "490 [D loss: 3.081069, acc.: 48.13%] [G loss: 4.421707]\n",
            "491 [D loss: 2.700534, acc.: 54.37%] [G loss: 4.077658]\n",
            "492 [D loss: 3.298371, acc.: 45.62%] [G loss: 2.992049]\n",
            "493 [D loss: 2.503004, acc.: 48.13%] [G loss: 3.786814]\n",
            "494 [D loss: 2.393968, acc.: 55.63%] [G loss: 3.060524]\n",
            "495 [D loss: 2.356048, acc.: 42.50%] [G loss: 3.065855]\n",
            "496 [D loss: 2.960179, acc.: 51.25%] [G loss: 2.644274]\n",
            "497 [D loss: 2.502952, acc.: 53.75%] [G loss: 3.967530]\n",
            "498 [D loss: 2.738932, acc.: 51.87%] [G loss: 2.544504]\n",
            "499 [D loss: 3.084802, acc.: 43.12%] [G loss: 3.451673]\n",
            "500 [D loss: 2.812605, acc.: 55.00%] [G loss: 3.040254]\n",
            "501 [D loss: 2.891160, acc.: 49.37%] [G loss: 2.753948]\n",
            "502 [D loss: 2.981441, acc.: 45.62%] [G loss: 2.732904]\n",
            "503 [D loss: 2.484567, acc.: 53.13%] [G loss: 4.154620]\n",
            "504 [D loss: 2.928563, acc.: 48.75%] [G loss: 2.585213]\n",
            "505 [D loss: 1.977643, acc.: 51.88%] [G loss: 3.885905]\n",
            "506 [D loss: 3.340659, acc.: 50.00%] [G loss: 3.312835]\n",
            "507 [D loss: 2.300683, acc.: 50.62%] [G loss: 4.235214]\n",
            "508 [D loss: 2.747500, acc.: 46.88%] [G loss: 3.429312]\n",
            "509 [D loss: 2.528859, acc.: 48.12%] [G loss: 3.768595]\n",
            "510 [D loss: 2.288440, acc.: 51.88%] [G loss: 3.269843]\n",
            "511 [D loss: 2.896355, acc.: 49.37%] [G loss: 2.761688]\n",
            "512 [D loss: 2.716893, acc.: 53.12%] [G loss: 2.920801]\n",
            "513 [D loss: 2.638119, acc.: 50.00%] [G loss: 3.340390]\n",
            "514 [D loss: 2.317345, acc.: 50.63%] [G loss: 3.438792]\n",
            "515 [D loss: 1.967973, acc.: 55.63%] [G loss: 3.319192]\n",
            "516 [D loss: 2.396623, acc.: 56.25%] [G loss: 2.845650]\n",
            "517 [D loss: 2.350667, acc.: 55.00%] [G loss: 3.567394]\n",
            "518 [D loss: 2.953073, acc.: 49.38%] [G loss: 2.441646]\n",
            "519 [D loss: 2.742846, acc.: 52.50%] [G loss: 2.571935]\n",
            "520 [D loss: 1.888305, acc.: 53.12%] [G loss: 3.045343]\n",
            "521 [D loss: 2.922900, acc.: 48.75%] [G loss: 3.649052]\n",
            "522 [D loss: 2.153480, acc.: 55.63%] [G loss: 2.005733]\n",
            "523 [D loss: 3.053776, acc.: 48.75%] [G loss: 2.661460]\n",
            "524 [D loss: 2.931611, acc.: 47.50%] [G loss: 3.021452]\n",
            "525 [D loss: 2.085599, acc.: 53.75%] [G loss: 2.305793]\n",
            "526 [D loss: 2.712400, acc.: 46.25%] [G loss: 3.186892]\n",
            "527 [D loss: 2.906448, acc.: 46.25%] [G loss: 4.173663]\n",
            "528 [D loss: 2.793813, acc.: 49.38%] [G loss: 3.450256]\n",
            "529 [D loss: 2.664602, acc.: 55.00%] [G loss: 2.843600]\n",
            "530 [D loss: 2.588428, acc.: 48.12%] [G loss: 3.220192]\n",
            "531 [D loss: 2.148970, acc.: 52.50%] [G loss: 2.739871]\n",
            "532 [D loss: 1.915601, acc.: 55.63%] [G loss: 3.898145]\n",
            "533 [D loss: 2.279499, acc.: 55.63%] [G loss: 5.198850]\n",
            "534 [D loss: 2.103767, acc.: 55.00%] [G loss: 3.602666]\n",
            "535 [D loss: 2.948455, acc.: 46.25%] [G loss: 2.537734]\n",
            "536 [D loss: 2.020122, acc.: 55.63%] [G loss: 3.479984]\n",
            "537 [D loss: 2.987180, acc.: 46.25%] [G loss: 3.213455]\n",
            "538 [D loss: 2.776993, acc.: 55.00%] [G loss: 3.919307]\n",
            "539 [D loss: 2.962590, acc.: 47.50%] [G loss: 2.185287]\n",
            "540 [D loss: 2.745667, acc.: 55.00%] [G loss: 2.636687]\n",
            "541 [D loss: 1.887851, acc.: 56.25%] [G loss: 3.783713]\n",
            "542 [D loss: 2.279237, acc.: 46.88%] [G loss: 3.271336]\n",
            "543 [D loss: 3.035285, acc.: 44.37%] [G loss: 3.190836]\n",
            "544 [D loss: 2.567466, acc.: 53.75%] [G loss: 2.724309]\n",
            "545 [D loss: 2.322636, acc.: 48.12%] [G loss: 2.717242]\n",
            "546 [D loss: 2.452400, acc.: 46.88%] [G loss: 3.174374]\n",
            "547 [D loss: 2.185158, acc.: 54.37%] [G loss: 3.794357]\n",
            "548 [D loss: 2.666459, acc.: 52.50%] [G loss: 4.226418]\n",
            "549 [D loss: 3.149061, acc.: 48.75%] [G loss: 1.438528]\n",
            "550 [D loss: 1.494731, acc.: 56.25%] [G loss: 4.099158]\n",
            "551 [D loss: 2.954587, acc.: 48.75%] [G loss: 2.763762]\n",
            "552 [D loss: 2.604343, acc.: 48.75%] [G loss: 2.801267]\n",
            "553 [D loss: 2.541614, acc.: 46.25%] [G loss: 2.489944]\n",
            "554 [D loss: 2.059832, acc.: 50.62%] [G loss: 2.971567]\n",
            "555 [D loss: 2.354529, acc.: 48.75%] [G loss: 3.131449]\n",
            "556 [D loss: 2.347572, acc.: 50.00%] [G loss: 2.649209]\n",
            "557 [D loss: 2.362334, acc.: 46.25%] [G loss: 1.669961]\n",
            "558 [D loss: 2.363912, acc.: 55.63%] [G loss: 3.299797]\n",
            "559 [D loss: 1.636009, acc.: 55.62%] [G loss: 2.929606]\n",
            "560 [D loss: 2.854322, acc.: 48.12%] [G loss: 2.756284]\n",
            "561 [D loss: 2.533980, acc.: 50.63%] [G loss: 2.756916]\n",
            "562 [D loss: 2.444779, acc.: 50.00%] [G loss: 1.975675]\n",
            "563 [D loss: 3.267382, acc.: 45.00%] [G loss: 2.696140]\n",
            "564 [D loss: 3.255344, acc.: 43.75%] [G loss: 1.821976]\n",
            "565 [D loss: 2.573258, acc.: 51.25%] [G loss: 2.956662]\n",
            "566 [D loss: 2.950742, acc.: 43.75%] [G loss: 1.809692]\n",
            "567 [D loss: 2.167792, acc.: 47.50%] [G loss: 2.293885]\n",
            "568 [D loss: 1.862005, acc.: 53.75%] [G loss: 1.927060]\n",
            "569 [D loss: 2.170126, acc.: 49.38%] [G loss: 2.174592]\n",
            "570 [D loss: 2.334977, acc.: 55.63%] [G loss: 1.612485]\n",
            "571 [D loss: 1.889179, acc.: 51.87%] [G loss: 2.605139]\n",
            "572 [D loss: 2.467906, acc.: 48.75%] [G loss: 2.005685]\n",
            "573 [D loss: 2.602336, acc.: 46.88%] [G loss: 2.118408]\n",
            "574 [D loss: 2.222149, acc.: 53.75%] [G loss: 1.473212]\n",
            "575 [D loss: 2.446853, acc.: 49.37%] [G loss: 2.570545]\n",
            "576 [D loss: 1.910356, acc.: 50.63%] [G loss: 1.899170]\n",
            "577 [D loss: 2.581041, acc.: 52.50%] [G loss: 3.617048]\n",
            "578 [D loss: 2.924879, acc.: 50.63%] [G loss: 1.924290]\n",
            "579 [D loss: 2.902205, acc.: 51.25%] [G loss: 2.012930]\n",
            "580 [D loss: 2.985431, acc.: 49.37%] [G loss: 2.432032]\n",
            "581 [D loss: 2.491527, acc.: 54.38%] [G loss: 2.158516]\n",
            "582 [D loss: 2.393532, acc.: 53.75%] [G loss: 1.852224]\n",
            "583 [D loss: 2.225686, acc.: 48.75%] [G loss: 2.711312]\n",
            "584 [D loss: 3.226986, acc.: 45.62%] [G loss: 2.592804]\n",
            "585 [D loss: 2.433174, acc.: 50.00%] [G loss: 2.691134]\n",
            "586 [D loss: 3.169958, acc.: 51.25%] [G loss: 2.090401]\n",
            "587 [D loss: 2.525054, acc.: 58.13%] [G loss: 2.646437]\n",
            "588 [D loss: 2.702387, acc.: 48.12%] [G loss: 2.387039]\n",
            "589 [D loss: 2.264656, acc.: 50.63%] [G loss: 2.718145]\n",
            "590 [D loss: 2.451963, acc.: 48.75%] [G loss: 2.286171]\n",
            "591 [D loss: 3.223386, acc.: 40.62%] [G loss: 1.878210]\n",
            "592 [D loss: 2.513049, acc.: 47.50%] [G loss: 2.266935]\n",
            "593 [D loss: 2.451938, acc.: 46.88%] [G loss: 1.384975]\n",
            "594 [D loss: 2.979326, acc.: 50.00%] [G loss: 2.348576]\n",
            "595 [D loss: 3.170326, acc.: 46.88%] [G loss: 2.778546]\n",
            "596 [D loss: 2.173103, acc.: 55.00%] [G loss: 1.811842]\n",
            "597 [D loss: 2.544642, acc.: 44.37%] [G loss: 2.253881]\n",
            "598 [D loss: 2.004192, acc.: 50.00%] [G loss: 1.883070]\n",
            "599 [D loss: 2.832672, acc.: 41.25%] [G loss: 1.968724]\n",
            "600 [D loss: 2.695077, acc.: 48.75%] [G loss: 2.181397]\n",
            "601 [D loss: 2.013249, acc.: 54.38%] [G loss: 1.863416]\n",
            "602 [D loss: 2.212554, acc.: 54.37%] [G loss: 2.446381]\n",
            "603 [D loss: 2.634383, acc.: 50.00%] [G loss: 1.878408]\n",
            "604 [D loss: 2.503786, acc.: 49.37%] [G loss: 2.854617]\n",
            "605 [D loss: 2.777703, acc.: 45.00%] [G loss: 2.145513]\n",
            "606 [D loss: 2.051947, acc.: 48.75%] [G loss: 2.881791]\n",
            "607 [D loss: 2.510931, acc.: 51.25%] [G loss: 2.871714]\n",
            "608 [D loss: 2.810841, acc.: 48.75%] [G loss: 3.117234]\n",
            "609 [D loss: 2.495328, acc.: 63.75%] [G loss: 3.513309]\n",
            "610 [D loss: 4.058999, acc.: 53.75%] [G loss: 2.759933]\n",
            "611 [D loss: 3.255933, acc.: 53.12%] [G loss: 4.694537]\n",
            "612 [D loss: 4.237499, acc.: 45.00%] [G loss: 3.022254]\n",
            "613 [D loss: 3.261835, acc.: 50.62%] [G loss: 3.193173]\n",
            "614 [D loss: 4.064131, acc.: 41.25%] [G loss: 3.062238]\n",
            "615 [D loss: 3.135127, acc.: 52.50%] [G loss: 3.492827]\n",
            "616 [D loss: 2.865096, acc.: 48.13%] [G loss: 1.889949]\n",
            "617 [D loss: 2.658189, acc.: 48.75%] [G loss: 3.663748]\n",
            "618 [D loss: 3.400956, acc.: 52.50%] [G loss: 3.855921]\n",
            "619 [D loss: 3.534635, acc.: 48.13%] [G loss: 3.052803]\n",
            "620 [D loss: 3.026909, acc.: 52.50%] [G loss: 3.550915]\n",
            "621 [D loss: 1.767440, acc.: 61.25%] [G loss: 3.135513]\n",
            "622 [D loss: 2.855292, acc.: 50.62%] [G loss: 2.582306]\n",
            "623 [D loss: 3.642229, acc.: 48.13%] [G loss: 3.818132]\n",
            "624 [D loss: 3.281590, acc.: 53.75%] [G loss: 5.221262]\n",
            "625 [D loss: 3.208353, acc.: 45.00%] [G loss: 3.663023]\n",
            "626 [D loss: 3.374869, acc.: 48.12%] [G loss: 2.326206]\n",
            "627 [D loss: 3.662138, acc.: 50.62%] [G loss: 3.575175]\n",
            "628 [D loss: 3.052852, acc.: 53.12%] [G loss: 2.680323]\n",
            "629 [D loss: 2.798551, acc.: 52.50%] [G loss: 3.918743]\n",
            "630 [D loss: 3.012333, acc.: 50.62%] [G loss: 3.453176]\n",
            "631 [D loss: 3.247561, acc.: 48.12%] [G loss: 3.594042]\n",
            "632 [D loss: 2.897018, acc.: 50.62%] [G loss: 3.585690]\n",
            "633 [D loss: 2.618792, acc.: 57.50%] [G loss: 3.486196]\n",
            "634 [D loss: 3.352388, acc.: 50.00%] [G loss: 4.522742]\n",
            "635 [D loss: 3.572445, acc.: 43.75%] [G loss: 4.470211]\n",
            "636 [D loss: 2.779696, acc.: 48.75%] [G loss: 4.539494]\n",
            "637 [D loss: 3.454229, acc.: 50.00%] [G loss: 4.005798]\n",
            "638 [D loss: 3.392356, acc.: 50.00%] [G loss: 3.817158]\n",
            "639 [D loss: 3.134766, acc.: 51.87%] [G loss: 4.786410]\n",
            "640 [D loss: 4.301832, acc.: 45.00%] [G loss: 4.225905]\n",
            "641 [D loss: 2.667478, acc.: 56.88%] [G loss: 2.074722]\n",
            "642 [D loss: 3.009755, acc.: 51.87%] [G loss: 5.230995]\n",
            "643 [D loss: 2.829753, acc.: 48.75%] [G loss: 4.221484]\n",
            "644 [D loss: 3.672437, acc.: 37.50%] [G loss: 3.657536]\n",
            "645 [D loss: 2.802279, acc.: 53.75%] [G loss: 4.259592]\n",
            "646 [D loss: 4.079066, acc.: 41.88%] [G loss: 3.616187]\n",
            "647 [D loss: 2.319164, acc.: 54.38%] [G loss: 3.691345]\n",
            "648 [D loss: 2.834994, acc.: 46.87%] [G loss: 3.845463]\n",
            "649 [D loss: 2.896194, acc.: 51.25%] [G loss: 4.505898]\n",
            "650 [D loss: 2.598307, acc.: 55.00%] [G loss: 3.921355]\n",
            "651 [D loss: 3.090716, acc.: 54.37%] [G loss: 3.566322]\n",
            "652 [D loss: 1.984533, acc.: 55.63%] [G loss: 3.499485]\n",
            "653 [D loss: 2.688996, acc.: 55.00%] [G loss: 3.986931]\n",
            "654 [D loss: 3.226828, acc.: 47.50%] [G loss: 4.934785]\n",
            "655 [D loss: 2.997854, acc.: 51.25%] [G loss: 2.845638]\n",
            "656 [D loss: 2.813330, acc.: 51.25%] [G loss: 3.345622]\n",
            "657 [D loss: 3.488804, acc.: 53.75%] [G loss: 3.861839]\n",
            "658 [D loss: 4.231355, acc.: 48.13%] [G loss: 2.729616]\n",
            "659 [D loss: 3.084141, acc.: 51.25%] [G loss: 3.139074]\n",
            "660 [D loss: 4.135206, acc.: 43.75%] [G loss: 3.875423]\n",
            "661 [D loss: 2.894731, acc.: 50.62%] [G loss: 3.049163]\n",
            "662 [D loss: 3.369714, acc.: 53.12%] [G loss: 4.414448]\n",
            "663 [D loss: 2.832427, acc.: 56.87%] [G loss: 4.095154]\n",
            "664 [D loss: 3.323364, acc.: 47.50%] [G loss: 3.189087]\n",
            "665 [D loss: 3.353448, acc.: 50.00%] [G loss: 3.585842]\n",
            "666 [D loss: 3.389248, acc.: 50.62%] [G loss: 3.408379]\n",
            "667 [D loss: 2.141615, acc.: 57.50%] [G loss: 3.708484]\n",
            "668 [D loss: 3.405375, acc.: 48.75%] [G loss: 3.269672]\n",
            "669 [D loss: 2.689449, acc.: 57.50%] [G loss: 4.647022]\n",
            "670 [D loss: 2.780791, acc.: 50.63%] [G loss: 2.895225]\n",
            "671 [D loss: 3.814349, acc.: 46.25%] [G loss: 3.782859]\n",
            "672 [D loss: 2.985815, acc.: 46.25%] [G loss: 2.531302]\n",
            "673 [D loss: 2.688107, acc.: 56.25%] [G loss: 3.309959]\n",
            "674 [D loss: 2.883997, acc.: 52.50%] [G loss: 4.027398]\n",
            "675 [D loss: 3.241461, acc.: 51.25%] [G loss: 3.917769]\n",
            "676 [D loss: 3.180012, acc.: 51.25%] [G loss: 4.498460]\n",
            "677 [D loss: 2.777085, acc.: 54.38%] [G loss: 4.184852]\n",
            "678 [D loss: 3.311171, acc.: 52.50%] [G loss: 3.976884]\n",
            "679 [D loss: 3.635132, acc.: 48.75%] [G loss: 3.211149]\n",
            "680 [D loss: 2.963543, acc.: 48.75%] [G loss: 4.227663]\n",
            "681 [D loss: 2.235168, acc.: 56.87%] [G loss: 3.559603]\n",
            "682 [D loss: 3.587687, acc.: 48.13%] [G loss: 4.188868]\n",
            "683 [D loss: 2.972994, acc.: 53.75%] [G loss: 3.522421]\n",
            "684 [D loss: 2.512769, acc.: 53.12%] [G loss: 3.464656]\n",
            "685 [D loss: 2.982466, acc.: 46.25%] [G loss: 2.321837]\n",
            "686 [D loss: 2.600809, acc.: 58.13%] [G loss: 2.956890]\n",
            "687 [D loss: 2.952055, acc.: 45.62%] [G loss: 2.433468]\n",
            "688 [D loss: 2.773553, acc.: 50.00%] [G loss: 2.977953]\n",
            "689 [D loss: 2.765688, acc.: 50.00%] [G loss: 3.729663]\n",
            "690 [D loss: 2.914104, acc.: 53.12%] [G loss: 1.685715]\n",
            "691 [D loss: 2.769264, acc.: 55.00%] [G loss: 3.686359]\n",
            "692 [D loss: 3.564502, acc.: 45.00%] [G loss: 2.943809]\n",
            "693 [D loss: 3.244134, acc.: 45.00%] [G loss: 2.575630]\n",
            "694 [D loss: 2.730922, acc.: 52.50%] [G loss: 3.530421]\n",
            "695 [D loss: 2.450922, acc.: 53.75%] [G loss: 2.968489]\n",
            "696 [D loss: 3.137542, acc.: 50.62%] [G loss: 2.797943]\n",
            "697 [D loss: 2.953410, acc.: 48.12%] [G loss: 2.857846]\n",
            "698 [D loss: 4.008664, acc.: 49.37%] [G loss: 1.712300]\n",
            "699 [D loss: 3.268650, acc.: 51.25%] [G loss: 3.104329]\n",
            "700 [D loss: 1.691760, acc.: 59.38%] [G loss: 3.588638]\n",
            "701 [D loss: 2.893669, acc.: 46.88%] [G loss: 3.428482]\n",
            "702 [D loss: 2.698305, acc.: 52.50%] [G loss: 3.226205]\n",
            "703 [D loss: 2.267735, acc.: 54.38%] [G loss: 1.923639]\n",
            "704 [D loss: 4.034408, acc.: 45.62%] [G loss: 2.930067]\n",
            "705 [D loss: 2.392023, acc.: 51.87%] [G loss: 3.222418]\n",
            "706 [D loss: 2.685601, acc.: 59.38%] [G loss: 1.999224]\n",
            "707 [D loss: 2.683601, acc.: 48.13%] [G loss: 3.037620]\n",
            "708 [D loss: 3.881597, acc.: 50.00%] [G loss: 2.994085]\n",
            "709 [D loss: 2.493703, acc.: 52.50%] [G loss: 2.245667]\n",
            "710 [D loss: 2.860847, acc.: 50.00%] [G loss: 2.727412]\n",
            "711 [D loss: 3.129884, acc.: 53.12%] [G loss: 3.099794]\n",
            "712 [D loss: 3.457010, acc.: 39.37%] [G loss: 2.785398]\n",
            "713 [D loss: 3.523859, acc.: 51.25%] [G loss: 3.262455]\n",
            "714 [D loss: 2.649006, acc.: 50.63%] [G loss: 2.197593]\n",
            "715 [D loss: 3.304102, acc.: 46.25%] [G loss: 4.289330]\n",
            "716 [D loss: 2.758585, acc.: 50.00%] [G loss: 2.914368]\n",
            "717 [D loss: 2.947342, acc.: 50.62%] [G loss: 3.199839]\n",
            "718 [D loss: 2.676247, acc.: 50.00%] [G loss: 2.481626]\n",
            "719 [D loss: 3.026552, acc.: 50.63%] [G loss: 2.925544]\n",
            "720 [D loss: 3.214151, acc.: 50.00%] [G loss: 2.562519]\n",
            "721 [D loss: 2.799207, acc.: 49.37%] [G loss: 2.732591]\n",
            "722 [D loss: 3.608576, acc.: 49.38%] [G loss: 3.398592]\n",
            "723 [D loss: 3.381296, acc.: 48.13%] [G loss: 3.382578]\n",
            "724 [D loss: 3.139560, acc.: 54.37%] [G loss: 3.709961]\n",
            "725 [D loss: 3.088594, acc.: 51.25%] [G loss: 2.531947]\n",
            "726 [D loss: 3.006367, acc.: 53.75%] [G loss: 3.698892]\n",
            "727 [D loss: 3.093483, acc.: 43.75%] [G loss: 4.696816]\n",
            "728 [D loss: 3.792268, acc.: 47.50%] [G loss: 3.753330]\n",
            "729 [D loss: 3.570699, acc.: 48.12%] [G loss: 2.762745]\n",
            "730 [D loss: 2.114694, acc.: 45.62%] [G loss: 2.050775]\n",
            "731 [D loss: 2.404563, acc.: 45.63%] [G loss: 4.569368]\n",
            "732 [D loss: 2.927707, acc.: 49.38%] [G loss: 3.606249]\n",
            "733 [D loss: 2.260162, acc.: 60.00%] [G loss: 3.475812]\n",
            "734 [D loss: 3.114369, acc.: 51.87%] [G loss: 3.676630]\n",
            "735 [D loss: 3.115490, acc.: 46.25%] [G loss: 2.964905]\n",
            "736 [D loss: 2.468629, acc.: 45.62%] [G loss: 3.528322]\n",
            "737 [D loss: 2.568519, acc.: 53.13%] [G loss: 3.040596]\n",
            "738 [D loss: 2.139069, acc.: 58.12%] [G loss: 3.520642]\n",
            "739 [D loss: 2.945184, acc.: 49.38%] [G loss: 2.310128]\n",
            "740 [D loss: 2.707446, acc.: 51.25%] [G loss: 3.792178]\n",
            "741 [D loss: 3.102533, acc.: 50.00%] [G loss: 3.647887]\n",
            "742 [D loss: 2.763774, acc.: 52.50%] [G loss: 4.694934]\n",
            "743 [D loss: 2.797417, acc.: 46.25%] [G loss: 4.665901]\n",
            "744 [D loss: 3.054917, acc.: 53.13%] [G loss: 2.824943]\n",
            "745 [D loss: 2.509437, acc.: 55.63%] [G loss: 4.840636]\n",
            "746 [D loss: 2.776308, acc.: 51.88%] [G loss: 2.806107]\n",
            "747 [D loss: 2.894825, acc.: 57.50%] [G loss: 2.648577]\n",
            "748 [D loss: 3.264359, acc.: 48.13%] [G loss: 4.661008]\n",
            "749 [D loss: 3.064578, acc.: 53.75%] [G loss: 5.421166]\n",
            "750 [D loss: 2.909101, acc.: 51.25%] [G loss: 5.115198]\n",
            "751 [D loss: 2.853414, acc.: 55.63%] [G loss: 3.799700]\n",
            "752 [D loss: 3.010205, acc.: 52.50%] [G loss: 2.522777]\n",
            "753 [D loss: 2.717793, acc.: 55.00%] [G loss: 2.659522]\n",
            "754 [D loss: 2.187031, acc.: 51.88%] [G loss: 5.322819]\n",
            "755 [D loss: 3.447028, acc.: 43.75%] [G loss: 3.814658]\n",
            "756 [D loss: 3.478175, acc.: 46.25%] [G loss: 2.639592]\n",
            "757 [D loss: 2.985866, acc.: 54.38%] [G loss: 2.842278]\n",
            "758 [D loss: 3.306704, acc.: 46.87%] [G loss: 3.630640]\n",
            "759 [D loss: 3.184729, acc.: 50.00%] [G loss: 4.319407]\n",
            "760 [D loss: 2.873030, acc.: 46.88%] [G loss: 3.656394]\n",
            "761 [D loss: 3.276160, acc.: 48.75%] [G loss: 3.521000]\n",
            "762 [D loss: 3.054656, acc.: 46.88%] [G loss: 4.016206]\n",
            "763 [D loss: 3.221566, acc.: 51.87%] [G loss: 3.508975]\n",
            "764 [D loss: 2.757480, acc.: 59.38%] [G loss: 4.330245]\n",
            "765 [D loss: 2.970813, acc.: 56.25%] [G loss: 3.754757]\n",
            "766 [D loss: 2.929502, acc.: 49.38%] [G loss: 4.042913]\n",
            "767 [D loss: 3.358585, acc.: 45.00%] [G loss: 5.076811]\n",
            "768 [D loss: 3.380439, acc.: 44.38%] [G loss: 3.629460]\n",
            "769 [D loss: 2.364139, acc.: 48.75%] [G loss: 2.789876]\n",
            "770 [D loss: 2.723727, acc.: 47.50%] [G loss: 3.306127]\n",
            "771 [D loss: 3.462234, acc.: 48.12%] [G loss: 3.064191]\n",
            "772 [D loss: 3.408424, acc.: 48.13%] [G loss: 2.778702]\n",
            "773 [D loss: 2.838366, acc.: 45.00%] [G loss: 4.208570]\n",
            "774 [D loss: 2.970598, acc.: 44.37%] [G loss: 2.954488]\n",
            "775 [D loss: 3.251932, acc.: 48.12%] [G loss: 4.150654]\n",
            "776 [D loss: 2.923823, acc.: 51.88%] [G loss: 4.795710]\n",
            "777 [D loss: 2.873880, acc.: 56.87%] [G loss: 2.166741]\n",
            "778 [D loss: 2.712856, acc.: 50.00%] [G loss: 3.721971]\n",
            "779 [D loss: 2.010193, acc.: 52.50%] [G loss: 4.730297]\n",
            "780 [D loss: 3.461695, acc.: 44.37%] [G loss: 4.169465]\n",
            "781 [D loss: 3.160243, acc.: 48.75%] [G loss: 2.958261]\n",
            "782 [D loss: 2.678151, acc.: 52.50%] [G loss: 3.591849]\n",
            "783 [D loss: 2.098981, acc.: 51.88%] [G loss: 2.794329]\n",
            "784 [D loss: 2.868980, acc.: 51.25%] [G loss: 3.794881]\n",
            "785 [D loss: 3.332503, acc.: 49.38%] [G loss: 3.573426]\n",
            "786 [D loss: 2.748340, acc.: 50.63%] [G loss: 3.485495]\n",
            "787 [D loss: 2.916943, acc.: 50.62%] [G loss: 4.946332]\n",
            "788 [D loss: 3.203195, acc.: 51.88%] [G loss: 4.834483]\n",
            "789 [D loss: 3.051906, acc.: 50.63%] [G loss: 5.198518]\n",
            "790 [D loss: 2.889058, acc.: 49.37%] [G loss: 4.110075]\n",
            "791 [D loss: 2.356522, acc.: 51.88%] [G loss: 3.641792]\n",
            "792 [D loss: 3.130320, acc.: 49.37%] [G loss: 3.785382]\n",
            "793 [D loss: 3.502322, acc.: 41.25%] [G loss: 3.179061]\n",
            "794 [D loss: 2.810296, acc.: 43.75%] [G loss: 4.637217]\n",
            "795 [D loss: 2.657195, acc.: 53.75%] [G loss: 4.588048]\n",
            "796 [D loss: 3.337250, acc.: 53.12%] [G loss: 3.774226]\n",
            "797 [D loss: 3.159001, acc.: 51.25%] [G loss: 2.807311]\n",
            "798 [D loss: 2.829264, acc.: 49.37%] [G loss: 4.637883]\n",
            "799 [D loss: 2.716325, acc.: 54.38%] [G loss: 3.098847]\n",
            "800 [D loss: 2.703612, acc.: 52.50%] [G loss: 2.743579]\n",
            "801 [D loss: 2.983630, acc.: 50.62%] [G loss: 3.779971]\n",
            "802 [D loss: 2.802278, acc.: 48.13%] [G loss: 3.806045]\n",
            "803 [D loss: 2.957567, acc.: 46.25%] [G loss: 5.077306]\n",
            "804 [D loss: 2.814946, acc.: 46.25%] [G loss: 4.765365]\n",
            "805 [D loss: 3.415603, acc.: 51.87%] [G loss: 3.512252]\n",
            "806 [D loss: 1.861562, acc.: 58.12%] [G loss: 3.484911]\n",
            "807 [D loss: 2.242824, acc.: 55.63%] [G loss: 3.815189]\n",
            "808 [D loss: 3.158180, acc.: 50.62%] [G loss: 3.057184]\n",
            "809 [D loss: 3.036392, acc.: 45.00%] [G loss: 3.664269]\n",
            "810 [D loss: 2.164957, acc.: 50.00%] [G loss: 5.801944]\n",
            "811 [D loss: 3.307281, acc.: 45.62%] [G loss: 4.593812]\n",
            "812 [D loss: 2.803502, acc.: 53.12%] [G loss: 4.172124]\n",
            "813 [D loss: 2.484927, acc.: 53.75%] [G loss: 4.509440]\n",
            "814 [D loss: 1.830426, acc.: 57.50%] [G loss: 4.568029]\n",
            "815 [D loss: 2.815716, acc.: 47.50%] [G loss: 4.639189]\n",
            "816 [D loss: 3.046379, acc.: 45.62%] [G loss: 3.770655]\n",
            "817 [D loss: 3.192746, acc.: 51.25%] [G loss: 3.343318]\n",
            "818 [D loss: 3.194596, acc.: 45.63%] [G loss: 3.596353]\n",
            "819 [D loss: 2.683133, acc.: 54.37%] [G loss: 3.690435]\n",
            "820 [D loss: 2.817122, acc.: 47.50%] [G loss: 3.112295]\n",
            "821 [D loss: 2.372134, acc.: 56.87%] [G loss: 2.445173]\n",
            "822 [D loss: 2.695096, acc.: 50.62%] [G loss: 4.119112]\n",
            "823 [D loss: 3.206048, acc.: 50.62%] [G loss: 3.541214]\n",
            "824 [D loss: 3.332262, acc.: 50.63%] [G loss: 4.178404]\n",
            "825 [D loss: 2.926543, acc.: 51.25%] [G loss: 4.475978]\n",
            "826 [D loss: 2.591576, acc.: 48.13%] [G loss: 3.068913]\n",
            "827 [D loss: 2.578616, acc.: 48.75%] [G loss: 2.888975]\n",
            "828 [D loss: 2.192821, acc.: 54.37%] [G loss: 4.973867]\n",
            "829 [D loss: 2.821321, acc.: 55.62%] [G loss: 3.358727]\n",
            "830 [D loss: 2.887515, acc.: 49.38%] [G loss: 3.106670]\n",
            "831 [D loss: 1.989741, acc.: 61.25%] [G loss: 3.742601]\n",
            "832 [D loss: 2.340934, acc.: 55.00%] [G loss: 4.811926]\n",
            "833 [D loss: 2.209915, acc.: 54.38%] [G loss: 4.189683]\n",
            "834 [D loss: 3.075098, acc.: 56.25%] [G loss: 4.490158]\n",
            "835 [D loss: 2.379038, acc.: 51.88%] [G loss: 3.193222]\n",
            "836 [D loss: 3.210194, acc.: 50.00%] [G loss: 4.281707]\n",
            "837 [D loss: 2.903434, acc.: 49.37%] [G loss: 5.252939]\n",
            "838 [D loss: 3.207056, acc.: 46.88%] [G loss: 3.834431]\n",
            "839 [D loss: 3.605587, acc.: 48.75%] [G loss: 4.346479]\n",
            "840 [D loss: 3.116497, acc.: 54.38%] [G loss: 4.342127]\n",
            "841 [D loss: 2.576589, acc.: 50.62%] [G loss: 4.093098]\n",
            "842 [D loss: 2.709351, acc.: 49.38%] [G loss: 4.738924]\n",
            "843 [D loss: 3.249435, acc.: 51.25%] [G loss: 4.447634]\n",
            "844 [D loss: 2.876686, acc.: 54.37%] [G loss: 4.895811]\n",
            "845 [D loss: 2.836558, acc.: 48.75%] [G loss: 4.042252]\n",
            "846 [D loss: 2.570128, acc.: 49.38%] [G loss: 3.263979]\n",
            "847 [D loss: 2.898641, acc.: 56.25%] [G loss: 4.138211]\n",
            "848 [D loss: 2.480810, acc.: 48.12%] [G loss: 4.726100]\n",
            "849 [D loss: 3.354178, acc.: 45.62%] [G loss: 5.419698]\n",
            "850 [D loss: 3.763032, acc.: 40.62%] [G loss: 4.296085]\n",
            "851 [D loss: 2.389213, acc.: 55.63%] [G loss: 4.381062]\n",
            "852 [D loss: 2.603463, acc.: 51.25%] [G loss: 4.170656]\n",
            "853 [D loss: 2.777267, acc.: 45.63%] [G loss: 3.119954]\n",
            "854 [D loss: 2.412056, acc.: 55.63%] [G loss: 3.814730]\n",
            "855 [D loss: 2.839929, acc.: 44.37%] [G loss: 4.099163]\n",
            "856 [D loss: 2.441050, acc.: 46.25%] [G loss: 4.286526]\n",
            "857 [D loss: 2.866497, acc.: 55.63%] [G loss: 3.005513]\n",
            "858 [D loss: 2.307270, acc.: 45.62%] [G loss: 3.355855]\n",
            "859 [D loss: 3.022841, acc.: 48.12%] [G loss: 2.818475]\n",
            "860 [D loss: 2.735840, acc.: 50.00%] [G loss: 4.298536]\n",
            "861 [D loss: 2.369983, acc.: 48.12%] [G loss: 2.149051]\n",
            "862 [D loss: 3.219925, acc.: 51.25%] [G loss: 2.813130]\n",
            "863 [D loss: 2.429222, acc.: 48.12%] [G loss: 3.193268]\n",
            "864 [D loss: 2.274294, acc.: 53.75%] [G loss: 2.783700]\n",
            "865 [D loss: 2.972916, acc.: 50.62%] [G loss: 3.594451]\n",
            "866 [D loss: 3.018641, acc.: 56.25%] [G loss: 2.658027]\n",
            "867 [D loss: 2.586990, acc.: 45.00%] [G loss: 2.123825]\n",
            "868 [D loss: 2.358944, acc.: 54.38%] [G loss: 4.130090]\n",
            "869 [D loss: 3.051088, acc.: 45.62%] [G loss: 3.889082]\n",
            "870 [D loss: 3.352131, acc.: 53.75%] [G loss: 3.726179]\n",
            "871 [D loss: 3.111756, acc.: 50.62%] [G loss: 3.169067]\n",
            "872 [D loss: 2.383765, acc.: 45.62%] [G loss: 2.886036]\n",
            "873 [D loss: 2.386523, acc.: 48.75%] [G loss: 4.829326]\n",
            "874 [D loss: 3.188273, acc.: 40.00%] [G loss: 3.031282]\n",
            "875 [D loss: 3.183906, acc.: 48.75%] [G loss: 3.310860]\n",
            "876 [D loss: 3.007336, acc.: 51.88%] [G loss: 3.515641]\n",
            "877 [D loss: 2.606557, acc.: 48.13%] [G loss: 3.059813]\n",
            "878 [D loss: 2.675105, acc.: 49.37%] [G loss: 3.551185]\n",
            "879 [D loss: 2.144549, acc.: 55.63%] [G loss: 3.333608]\n",
            "880 [D loss: 2.236636, acc.: 56.87%] [G loss: 3.515012]\n",
            "881 [D loss: 2.918343, acc.: 48.13%] [G loss: 2.931772]\n",
            "882 [D loss: 2.948871, acc.: 52.50%] [G loss: 4.225629]\n",
            "883 [D loss: 2.936219, acc.: 49.38%] [G loss: 4.625728]\n",
            "884 [D loss: 2.106688, acc.: 46.88%] [G loss: 3.344486]\n",
            "885 [D loss: 2.447971, acc.: 41.88%] [G loss: 3.184008]\n",
            "886 [D loss: 2.931638, acc.: 42.50%] [G loss: 3.177375]\n",
            "887 [D loss: 2.343860, acc.: 49.37%] [G loss: 2.753188]\n",
            "888 [D loss: 2.523991, acc.: 50.62%] [G loss: 3.278870]\n",
            "889 [D loss: 2.640344, acc.: 53.12%] [G loss: 3.630507]\n",
            "890 [D loss: 2.905471, acc.: 54.38%] [G loss: 3.110883]\n",
            "891 [D loss: 3.262262, acc.: 48.12%] [G loss: 2.493547]\n",
            "892 [D loss: 2.061590, acc.: 56.87%] [G loss: 3.638751]\n",
            "893 [D loss: 2.267635, acc.: 51.88%] [G loss: 4.085407]\n",
            "894 [D loss: 2.722769, acc.: 44.37%] [G loss: 4.079015]\n",
            "895 [D loss: 2.721174, acc.: 49.38%] [G loss: 4.818017]\n",
            "896 [D loss: 2.479248, acc.: 53.12%] [G loss: 2.527159]\n",
            "897 [D loss: 2.708187, acc.: 51.25%] [G loss: 3.451811]\n",
            "898 [D loss: 2.541315, acc.: 46.87%] [G loss: 2.523324]\n",
            "899 [D loss: 2.906600, acc.: 44.37%] [G loss: 2.393834]\n",
            "900 [D loss: 2.129332, acc.: 51.88%] [G loss: 2.491255]\n",
            "901 [D loss: 2.504894, acc.: 51.25%] [G loss: 2.756997]\n",
            "902 [D loss: 2.251690, acc.: 55.00%] [G loss: 2.685232]\n",
            "903 [D loss: 2.455355, acc.: 44.38%] [G loss: 3.478732]\n",
            "904 [D loss: 2.418986, acc.: 52.50%] [G loss: 3.580359]\n",
            "905 [D loss: 2.489847, acc.: 51.88%] [G loss: 3.101151]\n",
            "906 [D loss: 2.386355, acc.: 43.75%] [G loss: 4.128814]\n",
            "907 [D loss: 2.378330, acc.: 51.88%] [G loss: 4.166164]\n",
            "908 [D loss: 2.611082, acc.: 52.50%] [G loss: 2.810918]\n",
            "909 [D loss: 2.573442, acc.: 46.88%] [G loss: 2.918861]\n",
            "910 [D loss: 2.670329, acc.: 45.00%] [G loss: 4.079506]\n",
            "911 [D loss: 2.051224, acc.: 61.25%] [G loss: 2.147203]\n",
            "912 [D loss: 2.582157, acc.: 48.75%] [G loss: 3.219294]\n",
            "913 [D loss: 2.453491, acc.: 50.62%] [G loss: 3.035943]\n",
            "914 [D loss: 2.577280, acc.: 51.88%] [G loss: 2.889117]\n",
            "915 [D loss: 2.582938, acc.: 58.75%] [G loss: 2.892452]\n",
            "916 [D loss: 3.413693, acc.: 50.62%] [G loss: 2.549612]\n",
            "917 [D loss: 2.599503, acc.: 53.75%] [G loss: 3.671944]\n",
            "918 [D loss: 2.867019, acc.: 45.62%] [G loss: 2.739908]\n",
            "919 [D loss: 2.269204, acc.: 52.50%] [G loss: 3.170678]\n",
            "920 [D loss: 2.619838, acc.: 56.87%] [G loss: 3.271079]\n",
            "921 [D loss: 3.479272, acc.: 49.37%] [G loss: 3.114449]\n",
            "922 [D loss: 2.313531, acc.: 49.37%] [G loss: 3.239285]\n",
            "923 [D loss: 2.402679, acc.: 54.38%] [G loss: 2.845804]\n",
            "924 [D loss: 2.935591, acc.: 44.37%] [G loss: 4.266568]\n",
            "925 [D loss: 2.183147, acc.: 56.87%] [G loss: 2.429302]\n",
            "926 [D loss: 3.022367, acc.: 42.50%] [G loss: 2.190282]\n",
            "927 [D loss: 2.008667, acc.: 55.00%] [G loss: 2.381116]\n",
            "928 [D loss: 3.106796, acc.: 45.62%] [G loss: 2.384407]\n",
            "929 [D loss: 2.294074, acc.: 51.88%] [G loss: 2.136118]\n",
            "930 [D loss: 2.384683, acc.: 50.00%] [G loss: 2.722625]\n",
            "931 [D loss: 2.832464, acc.: 46.25%] [G loss: 2.992826]\n",
            "932 [D loss: 2.523567, acc.: 49.38%] [G loss: 1.267664]\n",
            "933 [D loss: 2.608201, acc.: 47.50%] [G loss: 1.772253]\n",
            "934 [D loss: 1.749493, acc.: 60.00%] [G loss: 2.431614]\n",
            "935 [D loss: 1.866560, acc.: 50.62%] [G loss: 2.944321]\n",
            "936 [D loss: 3.048763, acc.: 45.00%] [G loss: 2.674685]\n",
            "937 [D loss: 2.874943, acc.: 48.12%] [G loss: 3.318190]\n",
            "938 [D loss: 2.116233, acc.: 52.50%] [G loss: 1.354307]\n",
            "939 [D loss: 1.884632, acc.: 52.50%] [G loss: 3.221754]\n",
            "940 [D loss: 3.348902, acc.: 50.00%] [G loss: 2.265278]\n",
            "941 [D loss: 2.263124, acc.: 60.63%] [G loss: 1.622054]\n",
            "942 [D loss: 2.441099, acc.: 52.50%] [G loss: 2.622546]\n",
            "943 [D loss: 2.054191, acc.: 53.75%] [G loss: 1.542279]\n",
            "944 [D loss: 2.747955, acc.: 54.38%] [G loss: 2.072965]\n",
            "945 [D loss: 3.140540, acc.: 44.37%] [G loss: 1.898490]\n",
            "946 [D loss: 3.651280, acc.: 43.75%] [G loss: 2.783506]\n",
            "947 [D loss: 2.357953, acc.: 54.37%] [G loss: 3.121345]\n",
            "948 [D loss: 2.275616, acc.: 54.38%] [G loss: 2.528412]\n",
            "949 [D loss: 2.656144, acc.: 51.88%] [G loss: 2.613792]\n",
            "950 [D loss: 2.342272, acc.: 50.00%] [G loss: 3.136666]\n",
            "951 [D loss: 2.061359, acc.: 55.00%] [G loss: 3.089330]\n",
            "952 [D loss: 3.039846, acc.: 48.75%] [G loss: 1.802583]\n",
            "953 [D loss: 2.381667, acc.: 55.63%] [G loss: 3.364281]\n",
            "954 [D loss: 1.780675, acc.: 57.50%] [G loss: 3.521452]\n",
            "955 [D loss: 2.124841, acc.: 54.37%] [G loss: 2.917597]\n",
            "956 [D loss: 2.494280, acc.: 51.87%] [G loss: 2.810067]\n",
            "957 [D loss: 2.312613, acc.: 49.38%] [G loss: 2.134717]\n",
            "958 [D loss: 2.813991, acc.: 51.25%] [G loss: 2.590681]\n",
            "959 [D loss: 2.958712, acc.: 46.88%] [G loss: 1.991351]\n",
            "960 [D loss: 2.495018, acc.: 49.38%] [G loss: 2.643570]\n",
            "961 [D loss: 2.703755, acc.: 55.00%] [G loss: 2.733104]\n",
            "962 [D loss: 3.513526, acc.: 43.75%] [G loss: 1.838267]\n",
            "963 [D loss: 2.664383, acc.: 56.87%] [G loss: 2.846540]\n",
            "964 [D loss: 2.144565, acc.: 52.50%] [G loss: 3.170833]\n",
            "965 [D loss: 2.705960, acc.: 55.62%] [G loss: 3.248723]\n",
            "966 [D loss: 3.255292, acc.: 46.25%] [G loss: 2.344247]\n",
            "967 [D loss: 3.236156, acc.: 51.25%] [G loss: 3.129799]\n",
            "968 [D loss: 2.954715, acc.: 45.00%] [G loss: 2.348542]\n",
            "969 [D loss: 3.128749, acc.: 45.00%] [G loss: 3.432274]\n",
            "970 [D loss: 2.897288, acc.: 41.25%] [G loss: 1.985785]\n",
            "971 [D loss: 2.617220, acc.: 59.38%] [G loss: 3.381660]\n",
            "972 [D loss: 2.880664, acc.: 51.88%] [G loss: 2.040572]\n",
            "973 [D loss: 2.652262, acc.: 53.75%] [G loss: 1.150272]\n",
            "974 [D loss: 2.451656, acc.: 51.88%] [G loss: 2.439310]\n",
            "975 [D loss: 2.762804, acc.: 50.62%] [G loss: 2.820772]\n",
            "976 [D loss: 2.246987, acc.: 48.13%] [G loss: 2.773833]\n",
            "977 [D loss: 2.520610, acc.: 51.25%] [G loss: 1.971745]\n",
            "978 [D loss: 2.878564, acc.: 46.88%] [G loss: 1.602776]\n",
            "979 [D loss: 3.050235, acc.: 48.75%] [G loss: 2.643910]\n",
            "980 [D loss: 1.823432, acc.: 51.88%] [G loss: 1.626693]\n",
            "981 [D loss: 1.823554, acc.: 55.63%] [G loss: 2.414407]\n",
            "982 [D loss: 2.842503, acc.: 52.50%] [G loss: 1.936865]\n",
            "983 [D loss: 2.776587, acc.: 51.87%] [G loss: 1.733067]\n",
            "984 [D loss: 2.487724, acc.: 49.37%] [G loss: 2.180832]\n",
            "985 [D loss: 2.946205, acc.: 45.62%] [G loss: 2.728381]\n",
            "986 [D loss: 2.311570, acc.: 51.25%] [G loss: 2.344014]\n",
            "987 [D loss: 2.915501, acc.: 50.00%] [G loss: 2.093897]\n",
            "988 [D loss: 2.212267, acc.: 51.88%] [G loss: 2.020201]\n",
            "989 [D loss: 2.511838, acc.: 51.25%] [G loss: 3.303977]\n",
            "990 [D loss: 3.170221, acc.: 41.88%] [G loss: 2.143270]\n",
            "991 [D loss: 2.539938, acc.: 50.62%] [G loss: 2.334560]\n",
            "992 [D loss: 2.296609, acc.: 55.00%] [G loss: 3.087397]\n",
            "993 [D loss: 2.519001, acc.: 46.88%] [G loss: 2.061660]\n",
            "994 [D loss: 2.025896, acc.: 49.38%] [G loss: 1.803945]\n",
            "995 [D loss: 2.774363, acc.: 50.00%] [G loss: 1.126134]\n",
            "996 [D loss: 2.449598, acc.: 49.38%] [G loss: 3.414692]\n",
            "997 [D loss: 1.988077, acc.: 45.00%] [G loss: 2.835088]\n",
            "998 [D loss: 2.626909, acc.: 52.50%] [G loss: 1.813142]\n",
            "999 [D loss: 2.583538, acc.: 43.75%] [G loss: 2.281097]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6XWf-9ied4i",
        "outputId": "cff3500a-4266-4383-fbb6-67e51f62f158"
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model(r'/content/drive/MyDrive/AI Project/Models/5000_Epoch.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVbK8g70p1Tu"
      },
      "source": [
        "random = np.random.normal(.000005,.00001,(1,4,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIaOpeaonF9S"
      },
      "source": [
        "predict = model.predict(random)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "eA--4oUeGrKA"
      },
      "source": [
        "#normalization\n",
        "predict = predict * 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "zjuehwN3GrKA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "754246c6-11a3-4128-a275-826781ebda27"
      },
      "source": [
        "print(predict)\n",
        "predict.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[ 73.38678 ]\n",
            "  [ 76.2939  ]\n",
            "  [ 79.29709 ]\n",
            "  [ 82.42817 ]\n",
            "  [ 85.72317 ]\n",
            "  [ 89.215225]\n",
            "  [ 92.8423  ]\n",
            "  [ 96.57019 ]\n",
            "  [100.54338 ]\n",
            "  [104.70265 ]\n",
            "  [109.39954 ]\n",
            "  [114.313576]\n",
            "  [119.53083 ]\n",
            "  [124.88176 ]\n",
            "  [130.35994 ]\n",
            "  [135.8582  ]\n",
            "  [141.30405 ]\n",
            "  [146.64664 ]\n",
            "  [151.79082 ]\n",
            "  [156.65288 ]\n",
            "  [161.13487 ]\n",
            "  [165.24202 ]\n",
            "  [168.9553  ]\n",
            "  [172.23106 ]\n",
            "  [175.11234 ]\n",
            "  [177.62685 ]\n",
            "  [179.80775 ]\n",
            "  [181.68823 ]\n",
            "  [183.30241 ]\n",
            "  [184.68158 ]\n",
            "  [185.8543  ]\n",
            "  [186.84535 ]\n",
            "  [187.67885 ]\n",
            "  [188.37454 ]\n",
            "  [188.94955 ]\n",
            "  [189.41776 ]\n",
            "  [189.78482 ]\n",
            "  [190.06233 ]\n",
            "  [190.25911 ]\n",
            "  [190.37846 ]\n",
            "  [190.4212  ]\n",
            "  [190.38686 ]\n",
            "  [190.27182 ]\n",
            "  [190.07002 ]\n",
            "  [189.7725  ]\n",
            "  [189.36801 ]\n",
            "  [188.8365  ]\n",
            "  [188.17009 ]\n",
            "  [187.34624 ]\n",
            "  [186.31696 ]\n",
            "  [185.04268 ]\n",
            "  [183.47372 ]\n",
            "  [181.52208 ]\n",
            "  [179.11118 ]\n",
            "  [176.13443 ]\n",
            "  [172.60905 ]\n",
            "  [168.64453 ]\n",
            "  [163.88808 ]\n",
            "  [158.28294 ]\n",
            "  [152.10538 ]\n",
            "  [145.02632 ]\n",
            "  [137.562   ]\n",
            "  [130.2659  ]\n",
            "  [125.58556 ]]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 64, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "DFGsfWueGrKB"
      },
      "source": [
        "midler = MidiFile()\n",
        "track = MidiTrack()\n",
        "midler.tracks.append(track)\n",
        "track.append(Message('program_change', program=2, time=0))\n",
        "for x in range(64):\n",
        "    track.append(Message('note_on', note=int(predict[0][x][0]), velocity=64, time=20))\n",
        "    track.append(Message('note_off', note=int(predict[0][x][0]), velocity=64, time=20))\n",
        "    midler.save('/content/drive/MyDrive/AI Project/Result/new_song.mid')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}